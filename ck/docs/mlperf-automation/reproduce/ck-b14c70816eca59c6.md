# Reproducibility report: system setup

**Tags:** MLPerf inference v1.1; Image Classification; DSE (Pareto frontier): mobilenet-v3-large-minimalistic_224_1.0_uint8; TFLite; AWS c6gd.xlarge; Neoverse-N1; edge; open division

## Install system dependencies

Similar to RPi4:
* https://github.com/mlcommons/ck/blob/master/docs/mlperf-automation/platform/rpi4-ubuntu.md

## Check python version (3.7+)

```bash
python3 --version

> 3.7.10
```

## Install [CK automation framework](https://github.com/mlcommons/ck)

Tested with the version 2.5.8

```bash
python3 -m pip install ck -U
```

*You may need to restart your bash to initialize PATH to CK scripts.*

## Prepare virtual environment with common MLPerf components

*You can either install stable components for MLPerf v1.1 (can't be updated) 
or the latest development components that you can update at any time:*

### Stable components for MLPerf v1.1

```bash
ck pull repo:mlcommons@ck-venv --checkout=mlperf-1.1-mlcommons
ck create venv:mlperf-inference --template=mlperf-inference-1.1-tvm-mlcommons
```

See CK packages that will be automatically installed from the [mlperf-inference-1.1-tvm-mlcommons template](https://github.com/mlcommons/ck-venv/blob/master/venv.template/mlperf-inference-1.1-tvm-mlcommons/script.sh).

Find more about CK automation for virtual environments [here](https://github.com/mlcommons/ck-venv).

### Development (live) components for MLPerf

```bash
ck pull repo:mlcommons@ck-venv
ck create venv:mlperf-inference9 --template=mlperf-inference-1.1-tvm
```

See CK packages that will be automatically installed from the [mlperf-inference-1.1-tvm template](https://github.com/mlcommons/ck-venv/blob/master/venv.template/mlperf-inference-1.1-tvm/script.sh).


## Activate virtual environment

```bash
ck activate venv:mlperf-inference
```

## Check installed CK packages

```bash
ck show env
```

## Install CK repository with ML/MLPerf automations 

```bash
ck where repo:mlcommons@ck-mlops
ck pull repo:ck-mlperf-inference
```

## Install CK components

```bash
# Not necessary unless native cmake is very old or not installed
# ck install package --tags=tool,cmake,prebuilt,v3.21.1

ck install package --tags=lib,mlperf,loadgen,static

ck install package --tags=lib,tflite,via-cmake,v2.5.0,with.ruy --j=4
```

## Prepare image classification task

Install ImageNet aux via CK:

```bash
ck install package --tags=imagenet,2012,aux
```

### Plug local ImageNet into CK

Note that the ImageNet validation dataset is not available for public download. 
Please follow these [instructions](https://github.com/mlcommons/ck/blob/master/docs/mlperf-automation/datasets/imagenet2012.md) to obtain it.

Find the path with the ImageNet validation set (50,000 images) and plug it into CK as follows:

```bash
ck detect soft:dataset.imagenet.val --force_version=2012 --extra_tags=full \
      --search_dir={directory where the ImageNet val dataset is downloaded}
```

Alternatively, you can install reduced ImageNet (500 images) via CK just to test your installation and CK workflows:
```bash
ck install package --tags=imagenet,2012,val,min,non-resized
```

### Preprocess ImageNet to 224x224 format

```bash
time ck install package --tags=dataset,imagenet,val,preprocessed,using-opencv,side.224 \
          --version=2012
```




### See all installed packages and detected components

```bash
ck show env
ck show env --tags=mlcommons,src
ck show env --tags=tflite
```



                                                                                                                                                                                      # Reproducibility report: design space exploration

This submission is a part of our collaboration with MLCommons&trade; to make it easier
to run MLPerf&trade; benchmark and visualize results on a Pareto frontier across different 
ML frameworks, models, datasets, compilers and platforms. 

We demonstrate the automated DSE process using our open-source [Collective Knowledge framework v2.5+](https://github.com/mlcommons/ck)
that includes a prototype of the [multi-dimensional (Pareto) frontier detector](https://github.com/mlcommons/ck-mlops/blob/main/module/bench.mlperf.inference/README.results.md)
to compact raw MLPerf results produced by older CK versions 1.x and reduce the number of sub-optimal submissions.

Do not forget to prepare your system based on the "Reproducibility report: system setup".


## Install TFLite model (MobileNet v2; Large; Minimalistic; 224; 1.0; uint8)

```bash
ck install package --tags=model,image-classification,tflite,mobilenet-v3,v3-large-minimalistic_224_1.0_uint8
```
More information about this model: 
[ [CK meta.json](https://github.com/mlcommons/ck-mlops/blob/main/package/model-tf-and-tflite-mlperf-mobilenet-v3/.cm/meta.json) ]
[ [summary](https://cknow.io/c/package/model-tf-and-tflite-mlperf-mobilenet-v3) ]


## SingleStream scenario

Since it's too long to run "offline" scenario and we can't satisfy min number of samples (24K), 
we run only SingleStream scenario and use it for the "offline" results.


### Accuracy

```bash
time ck benchmark program:image-classification-tflite-loadgen \
      --env.CK_LOADGEN_MODE=AccuracyOnly \
      --env.CK_LOADGEN_SCENARIO=SingleStream \
      --dep_add_tags.images=preprocessed,using-opencv,side.224 \
      --dep_add_tags.weights=v3-large-minimalistic_224_1.0_uint8 \
      --env.CK_LOADGEN_TARGET_LATENCY=5 \
      --env.CK_LOADGEN_BUFFER_SIZE=1024
```

### Performance

```bash
time ck benchmark program:image-classification-tflite-loadgen \
      --env.CK_LOADGEN_MODE=PerformanceOnly \
      --env.CK_LOADGEN_SCENARIO=SingleStream \
      --dep_add_tags.images=preprocessed,using-opencv,side.224 \
      --dep_add_tags.weights=v3-large-minimalistic_224_1.0_uint8 \
      --env.CK_LOADGEN_TARGET_LATENCY=5 \
      --env.CK_LOADGEN_BUFFER_SIZE=1024
```






## Prepare your submission

One can use the [end-to-end MLPerf submission and visualization workflow](https://github.com/mlcommons/ck-mlops/tree/main/module/bench.mlperf.inference)
(collaboration between MLCommons, OctoML and the cTuning foundation) to prepare the above submission as follows:

```bash

ck activate venv:mlperf-inference

ck pull repo:ck-mlperf-inference

ck install package --tags=mlperf,inference,results,dummy

ck set kernel --var.mlperf_inference_version=1.1
ck set kernel --var.mlperf_inference_submitter=cTuning
ck set kernel --var.mlperf_inference_division=open

ck add bench.mlperf.system:aws-c6gd.xlarge-arm64-tflite-2.5.0-ruy --base=rpi4-tflite-v2.2.0-ruy
ck set kernel --var.mlperf_inference_system=aws-c6gd.xlarge-arm64-tflite-2.5.0-ruy



time ck run bench.mlperf.inference --framework=tflite --model=mobilenet-v3-large-minimalistic_224_1.0_uint8 --scenario=singlestream --mode=accuracy \
      --skip_system_ext \
      --clean \
      --duplicate_to_offline \
      --workflow=image-classification-tflite-loadgen \
      --workflow_cmd=default \
      --task=image-classification \
      --env.CK_LOADGEN_MODE=AccuracyOnly \
      --env.CK_LOADGEN_SCENARIO=SingleStream \
      --dep_add_tags.images=preprocessed,using-opencv \
      --dep_add_tags.weights=v3-large-minimalistic_224_1.0_uint8 \
      --env.CK_LOADGEN_TARGET_LATENCY=5 \
      --env.CK_LOADGEN_BUFFER_SIZE=1024

time ck run bench.mlperf.inference --framework=tflite --model=mobilenet-v3-large-minimalistic_224_1.0_uint8 --scenario=singlestream --mode=performance \
      --skip_system_ext \
      --clean \
      --duplicate_to_offline \
      --workflow=image-classification-tflite-loadgen \
      --workflow_cmd=default \
      --task=image-classification \
      --env.CK_LOADGEN_MODE=PerformanceOnly \
      --env.CK_LOADGEN_SCENARIO=SingleStream \
      --dep_add_tags.images=preprocessed,using-opencv \
      --dep_add_tags.weights=v3-large-minimalistic_224_1.0_uint8 \
      --env.CK_LOADGEN_TARGET_LATENCY=5 \
      --env.CK_LOADGEN_BUFFER_SIZE=1024
```

You should truncate your accuracy files before submitting results:
```bash
ck run program:mlperf-inference-submission --cmd_key=truncate_accuracy_log --env.CK_MLPERF_SUBMITTER=cTuning
```

Check the submission by the MLPerf submission checker:
```bash
ck run program:mlperf-inference-submission --cmd_key=check
```

Pack results:
```
ck zip bench.mlperf.system
```



# Visualize MLPerf results

## Convert MLPerf inference results into the CK database format

* Supported host OS to process results: Linux, Windows, MacOS

## Install CK automation for the Python virtual environment

Install CK as described [here](https://github.com/ctuning/ck#installation).

Prepare virtual CK environment:

```bash
ck pull repo:mlcommons@ck-venv

ck create venv:mlperf --template=generic

ck activate venv:mlperf
```

## Pull CK repo with MLOps automation recipes
```bash
ck pull repo:mlcommons@ck-mlops
```

## Pull already processed results
```bash
ck pull repo:ck-mlperf-inference
```

## Install CK packages with MLPerf inference results

```bash
ck install package --tags=mlperf,inference,results,v1.1
ck install package --tags=mlperf,inference,results,v1.0
ck install package --tags=mlperf,inference,results,v0.7
ck install package --tags=mlperf,inference,results,v0.5
```

*Note that you can skip the above step and register your own local directory with your results as follows:*
```bash
ck detect soft:mlperf.inference.results  --full_path={FULL PATH TO YOUR DIRECTORY WITH MLPERF RESULTS}/README.md --force_version={SOME VERSION}
```

## Import results into CK format

Create a scratch CK repository to record results (unless you want to record them to the shared "ck-mlperf-inference" repo:
```bash
ck add repo:ck-mlperf-inference-1.1-dse --quiet
```

Import results to this repository:

```bash
ck import bench.mlperf.inference --target_repo=ck-mlperf-inference-1.1-dse
```

Note that we do not yet have a smart way to update results in the repository and they will be appended each time you run the above command.
You may want to clean your results before running this command again:
```bash
ck rm ck-mlperf-inference-1.1-dse:result:* -f
```

## Display all results locally

```bash
ck display dashboard --template=result --cfg=mlperf.inference.all
```

You can customize URL to show results from a given repo (ck_repo) and for a given scenario as follows:

* http://localhost:3344/?template=result&cfg=mlperf.inference.all&cfg_id=mlperf-inference-all-image-classification-edge-singlestream&repo_uoa=ck-mlperf-inference-1.1-dse
* http://localhost:3344/?template=result&cfg=mlperf.inference.all&cfg_id=mlperf-inference-all-image-classification-edge-offline&repo_uoa=ck-mlperf-inference-1.1-dse

You can also specify a given MLPerf scenario and a repository with results from the command line with starting a dashboard (CK version > 2.5.8):

```bash
ck display dashboard --template=result --cfg=mlperf.inference.all --cfg_id=mlperf-inference-all-image-classification-edge-singlestream --repo_uoa=ck-mlperf-inference-1.1-dse
```
or
```bash
ck display dashboard --template=result --cfg=mlperf.inference.all --extra_url="cfg_id=mlperf-inference-all-image-classification-edge-singlestream&repo_uoa=ck-mlperf-inference-1.1-dse"
```

## Process results on a Pareto frontier

```bash
ck filter ck-mlperf-inference-1.1-dse:bench.mlperf.inference:mlperf-inference-all-image-classification-edge-singlestream-pareto 
ck filter ck-mlperf-inference-1.1-dse:bench.mlperf.inference:mlperf-inference-all-*-pareto 
```

You can remove "ck-mlperf-inference-1.1-dse:" from above commands to process results in all repositories.


## Display other reproduced results at cKnowledge.io

* [List dashboards](https://cknow.io/reproduced-results)






## Resources

* [MLPerf Inference Benchmark article](https://arxiv.org/abs/1911.02549)
* [Motivation](https://www.youtube.com/watch?v=7zpeIVwICa4)
* [MLCommons working groups](https://mlcommons.org/en/groups)
* [Open-source CK framework](https://github.com/mlcommons/ck) and [MLCube](https://github.com/mlcommons/mlcube)
  * [ML/AI packages from the community, OctoML and the cTuning foundation](https://github.com/mlcommons/ck-mlops/tree/main/package)
  * [ML/AI workflows from the community, OctoML and the cTuning foundation](https://github.com/mlcommons/ck-mlops/tree/main/program)
* [Documentation for the CK-powered MLPerf automation suite](https://github.com/mlcommons/ck/tree/master/docs/mlperf-automation)
* [Prototype of the end-to-end submission workflow for the MLPerf inference benchmark](https://github.com/mlcommons/ck-mlops/tree/main/module/bench.mlperf.inference)
