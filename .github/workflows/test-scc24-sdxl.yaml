name: MLPerf inference SDXL (SCC)

on:
  schedule:
    - cron: "08 18 * * *"

jobs:
  build_reference:
    if: github.repository_owner == 'gateoverflow'
    runs-on: [ self-hosted, linux, x64, GO-spr ]
    env:
      CM_REPOS: $HOME/GH_CM
    strategy:
      fail-fast: false
      matrix:
        python-version: [ "3.12" ]
        backend: [ "pytorch" ]
        precision: [ "float16" ]
        device: [ "cuda" ]
    steps:
    - name: Test MLPerf Inference reference SDXL SCC 
      run: |
        if [ -f "gh_action/bin/deactivate" ]; then source gh_action/bin/deactivate; fi
        python3 -m venv gh_action
        source gh_action/bin/activate
        export CM_REPOS=$HOME/GH_CM
        pip install --upgrade cm4mlops
        pip install tabulate
        cm pull repo
        cm run script --tags=run-mlperf,inference,_find-performance,_r4.1-dev,_short,_scc24-base --adr.mlperf-implementation.tags=_branch.dev --pull_changes=yes --model=sdxl --implementation=reference --backend=${{ matrix.backend }} --category=datacenter --scenario=Offline --execution_mode=test --device=${{ matrix.device }} --precision=${{ matrix.precision }} --docker --docker_it=no --docker_cm_repo=gateoverflow@cm4mlops --docker_dt=yes --quiet --results_dir=$HOME/scc_gh_action_results --submission_dir=$HOME/scc_gh_action_submissions --precision=float16 --env.CM_MLPERF_MODEL_SDXL_DOWNLOAD_TO_HOST=yes --clean
        cm run script --tags=run-mlperf,inference,_r4.1-dev,_short,_scc24-base --adr.mlperf-implementation.tags=_branch.dev --model=sdxl --implementation=reference --backend=${{ matrix.backend }} --category=datacenter --scenario=Offline --execution_mode=test --device=${{ matrix.device }} --precision=${{ matrix.precision }} --docker --docker_it=no --docker_cm_repo=gateoverflow@cm4mlops --docker_dt=yes --quiet --results_dir=$HOME/scc_gh_action_results --submission_dir=$HOME/scc_gh_action_submissions --precision=float16 --env.CM_MLPERF_MODEL_SDXL_DOWNLOAD_TO_HOST=yes --clean
        cm run script --tags=generate,inference,submission --clean  --run-checker --tar=yes --env.CM_TAR_OUTFILE=submission.tar.gz --division=open --category=datacenter --run_style=test --adr.submission-checker.tags=_short-run --quiet --submitter=MLCommons --submission_dir=$HOME/scc_gh_action_submissions --results_dir=$HOME/scc_gh_action_results/test_results
        cm run script --tags=push,github,mlperf,inference,submission --repo_url=https://github.com/gateoverflow/cm4mlperf-inference --repo_branch=mlperf-inference-results-scc24 --commit_message="Results from self hosted Github actions - NVIDIARTX4090" --quiet --submission_dir=$HOME/scc_gh_action_submissions
        
  build_nvidia:
      if: github.repository_owner == 'gateoverflow'
      runs-on: [ self-hosted, linux, x64, GO-spr]
      strategy:
        fail-fast: false
        matrix:
          python-version: [ "3.12" ]
          backend: [ "tensorrt" ]
          precision: [ "float16" ]
          implementation: [ "nvidia" ]
      steps:
      - name: Test MLPerf Inference NVIDIA SDXL SCC
        run: |
          if [ -f "gh_action/bin/deactivate" ]; then source gh_action/bin/deactivate; fi
          python3 -m venv gh_action
          source gh_action/bin/activate
          export CM_REPOS=$HOME/GH_CM
          pip install --upgrade cm4mlops
          pip install tabulate
          cm pull repo
          cm run script --tags=run-mlperf,inference,_find-performance,_r4.1-dev,_short,_scc24-base --adr.mlperf-implementation.tags=_branch.dev --pull_changes=yes --model=sdxl --implementation=nvidia --backend=${{ matrix.backend }} --category=datacenter --scenario=Offline --execution_mode=test --device=${{ matrix.device }} --precision=${{ matrix.precision }} --docker --docker_it=no --docker_cm_repo=gateoverflow@cm4mlops --docker_dt=yes --pull_changes --quiet --results_dir=$HOME/scc_gh_action_results --submission_dir=$HOME/scc_gh_action_submissions --env.CM_MLPERF_MODEL_SDXL_DOWNLOAD_TO_HOST=yes --hw_name=go-spr --custom_system_nvidia=yes --clean
          cm run script --tags=run-mlperf,inference,_r4.1-dev,_short,_scc24-base --model=sdxl --adr.mlperf-implementation.tags=_branch.dev --implementation=nvidia --backend=${{ matrix.backend }} --category=datacenter --scenario=Offline --execution_mode=test --device=${{ matrix.device }} --precision=${{ matrix.precision }} --docker --docker_it=no --docker_cm_repo=gateoverflow@cm4mlops --docker_dt=yes --quiet --results_dir=$HOME/scc_gh_action_results --submission_dir=$HOME/scc_gh_action_submissions --precision=float16 --env.CM_MLPERF_MODEL_SDXL_DOWNLOAD_TO_HOST=yes --clean
          cm run script --tags=generate,inference,submission --clean --preprocess_submission=yes --run-checker --tar=yes --env.CM_TAR_OUTFILE=submission.tar.gz --division=open --category=datacenter --run_style=test --adr.submission-checker.tags=_short-run --quiet --submitter=MLCommons --submission_dir=$HOME/scc_gh_action_submissions --results_dir=$HOME/scc_gh_action_results/test_results
          cm run script --tags=push,github,mlperf,inference,submission --repo_url=https://github.com/gateoverflow/cm4mlperf-inference --repo_branch=mlperf-inference-results-scc24 --commit_message="Results from self hosted Github actions - NVIDIARTX4090" --quiet --submission_dir=$HOME/scc_gh_action_submissions
