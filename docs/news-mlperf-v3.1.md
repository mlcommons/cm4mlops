[ [Back to index](README.md) ]

This section highlights the new CM capabilities to [modularize and automate MLPerf inference benchmarks](https://github.com/mlcommons/ck/tree/master/docs/mlperf)
added by the [cTuning foundation](https://cTuning.org), [cKnowledge](https://www.linkedin.com/company/cknowledge)
and [the community](https://access.cknowledge.org/playground/?action=contributors) 
via our [public MLPerf challenges](https://access.cknowledge.org/playground/?action=challenges).

We thank Neural Magic, TTA, One Stop Systems, Nutanix, Collabora, Deelvin, cKnowledge, AMD and Nvidia
for interesting discussions and feedback that helped us improve CM automation for MLPerf benchmarks and beyond.


## New CM automation capabilities for MLPerf inference benchmarks


We showcase One Stop Systems Rigel Edge Super Computer which has many interesting performance results.

### MLperf benchmark implementations

* DeepSparse from Neural Magic
* Nvidia optimized implementations
* All reference implementations including GPT-J
* 

Extensible support for different MLPerf implementations and backends from different vendors including DeepSparse, Nvidia, Intel and Qualcomm.
Support for DeepSparse, NCNN, ONNX, TensorRT, PyTorch, TensorFlow, TFLite, TVM, Triton (CPU and accelerators)
Support for Coral TPU, Nvidia GPUs (A100,T4,L4,RTX 4090, Orin), Intel/AMD servers, Graviton, NeoVerse, Apple metal
Support for all MLPerf models including GPT-J
Support for AWS, GCP, Azure with Ubuntu, RHEL, SLES, Amazon Linux and Windows 11
Submissions across numerous laptops provided by the community including Apple, Dell, Toshiba, Asus, HPE, Lenovo


### Support for model zoos

### Support for data sets

Croissant

### Support for models 


BERT DSE
Hugging Face

### Support for MLPerf inference backends

### Support for ML frameworks

### Support for Operating Systems

### Support for systems

###




## Highlights of the MLPerf inference v3.1 results from the community and cTuning

*These information will become public with the official release of the MLPerf inference v3.1 results.*
