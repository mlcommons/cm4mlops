[ [Back to index](README.md) ]

# Run MLPerf inference benchmarks out-of-the-box

This documentation will help you run, reproduce and compare MLPerf inference benchmarks out-of-the-box 
across any software, hardware, models and data sets from any vendor
using the open-source and technology-agnostic [MLCommons Collective Mind automation language (CM)](https://doi.org/10.5281/zenodo.8105339)
and [MLCommons Collective Knowledge Playground (CK)](https://access.cknowledge.org/playground/?action=experiments).

This project is supported by the [MLCommons Task Force on Automation and Reproducibility](../taskforce.md),
[cTuning foundation](https://cTuning.org) and [cKnowledge Ltd](https://cKnowledge.org).

Don't hesitate to get in touch with us using this [public Discord server](https://discord.gg/JjWNWXKxwT) 
to provide your feedback, ask questions, add new benchmark implementations, models, data sets and hardware backends,
and prepare and optimize your MLPerf submissions.

# Setup

TBD



