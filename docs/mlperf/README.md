[ [Back to index](README.md) ]

This documentation will help you run, reproduce and compare MLPerf benchmarks out-of-the-box 
across any software, hardware, models and data sets from any vendor
using the open-source and technology-agnostic [MLCommons Collective Mind automation language (CM)](https://doi.org/10.5281/zenodo.8105339)
and [MLCommons Collective Knowledge Playground (CK)](https://access.cknowledge.org/playground/?action=experiments).

Please choose which benchmark you want to run:
* [MLPerf inference](inference/README.md)
* [MLPerf training](../tutorials/reproduce-mlperf-training.md) *(prototyping phase)*
* [MLPerf tiny](../tutorials/reproduce-mlperf-tiny.md) *(prototyping phase)*
* MLPerf mobile *(preparation phase)*

This project is supported by the [MLCommons Task Force on Automation and Reproducibility](../taskforce.md),
[cTuning foundation](https://cTuning.org) and [cKnowledge Ltd](https://cKnowledge.org).

Don't hesitate to get in touch with us using this [public Discord server](https://discord.gg/JjWNWXKxwT) 
to provide your feedback, ask questions, add new benchmark implementations, models, data sets and hardware backends,
and prepare and optimize your MLPerf submissions.
