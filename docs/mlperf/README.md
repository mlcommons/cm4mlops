[ [Back to index](README.md) ]

# Run MLPerf inference benchmarks out-of-the-box

This documentation will help you run, reproduce and compare MLPerf benchmarks out-of-the-box 
across any software, hardware, models and data sets from any vendor
using the open-source and technology-agnostic [MLCommons Collective Mind automation language (CM)](https://doi.org/10.5281/zenodo.8105339)
and [MLCommons Collective Knowledge Playground (CK)](https://access.cknowledge.org/playground/?action=experiments).

Please choose which benchmark you want to run:
* [MLPerf inference](inference/README.md)
* [MLPerf training](../tutorials/reproduce-mlperf-training.md) *(prototyping phase)*
* [MLPerf tiny](../tutorials/reproduce-mlperf-tiny.md) *(prototyping phase)*
* MLPerf mobile *(preparation phase)*

This project is under heavy development led by [Grigori Fursin](https://cKnowledge.org/gfursin) and [Arjun Suresh](https://www.linkedin.com/in/arjunsuresh)
and supported by the [MLCommons Task Force on Automation and Reproducibility](../taskforce.md),
[cTuning.org](https://cTuning.org) and [cKnowledge.org](https://cKnowledge.org).
You can learn more about our vision and plans from our [ACM REP'23 keynote](https://doi.org/10.5281/zenodo.8105339).

Don't hesitate to get in touch with us using this [public Discord server](https://discord.gg/JjWNWXKxwT) 
to provide your feedback, ask questions, add new benchmark implementations, models, data sets and hardware backends,
prepare and optimize your MLPerf submissions and participate in our [reproducibility and optimization challenges](https://access.cknowledge.org/playground/?action=challenges).

You can learn more about our vision and plans from our [ACM REP keynote (June 2023)](https://doi.org/10.5281/zenodo.8105339).
