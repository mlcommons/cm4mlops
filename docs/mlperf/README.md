[ [Back to CM documentation](../README.md) ]

# Run MLPerf benchmarks out-of-the-box

This documentation will help you run, reproduce and compare MLPerf benchmarks out-of-the-box 
in a unified way across different software, hardware, models and data sets using 
the the [MLCommons Collective Mind automation language (CM)](https://doi.org/10.5281/zenodo.8105339).

Please choose which benchmark you want to run:
* [MLPerf inference benchmark](inference/README.md)
* [MLPerf training benchmark](../tutorials/reproduce-mlperf-training.md) *(prototyping phase)*
* [MLPerf tiny benchmark](../tutorials/reproduce-mlperf-tiny.md) *(prototyping phase)*
* MLPerf mobile *(preparation phase)*

This project is under development by the [MLCommons Task Force on Automation and Reproducibility](../taskforce.md),
[cTuning.org](https://cTuning.org) and [cKnowledge.org](https://cKnowledge.org) - don't hesitate to get in touch 
via the [public Discord server](https://discord.gg/JjWNWXKxwT).
