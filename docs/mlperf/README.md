[ [Back to CM documentation](../README.md) ]

# Run and customize MLPerf benchmarks using the MLCommons CM automation framework

This documentation explains how to compose, run, customize and extend MLPerf benchmarks 
in a unified way across diverse models, data sets, software and hardware from different vendors 
using [MLCommons Collective Mind automation recipes](https://access.cknowledge.org/playground/?action=scripts):

* [MLPerf inference benchmark](inference/README.md)
* [MLPerf training benchmark](../tutorials/reproduce-mlperf-training.md) *(prototyping phase)*
* [MLPerf tiny benchmark](../tutorials/reproduce-mlperf-tiny.md) *(prototyping phase)*
* MLPerf automotive *(prototyping phase)*
* MLPerf mobile *(preparation phase)*
* MLPerf client *(preparation phase)*

*Note that the [MLCommons Task Force on Automation and Reproducibility](../taskforce.md)
 is preparing a [GUI](https://access.cknowledge.org/playground/?action=howtorun) 
 to make it easier to run, customize, reproduce and compare
 MLPerf benchmarks - please stay tuned for more details!*

Don't hesitate to get in touch via the [public Discord server](https://discord.gg/JjWNWXKxwT) if you have questions or feedback!
