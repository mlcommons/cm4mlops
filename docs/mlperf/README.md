[ [Back to CM documentation](../README.md) ]

# Run MLPerf inference benchmarks out-of-the-box

This documentation will help you run, reproduce and compare MLPerf benchmarks out-of-the-box 
across different software, hardware, models and data sets using 
the open-source and technology-agnostic [MLCommons Collective Mind automation language (CM)](https://doi.org/10.5281/zenodo.8105339)
and [MLCommons Collective Knowledge Playground (CK)](https://access.cknowledge.org/playground/?action=experiments).

Please choose which benchmark you want to run:
* [MLPerf inference](inference/README.md)
* [MLPerf training](../tutorials/reproduce-mlperf-training.md) *(prototyping phase)*
* [MLPerf tiny](../tutorials/reproduce-mlperf-tiny.md) *(prototyping phase)*
* MLPerf mobile *(preparation phase)*

This project is under heavy development by the [MLCommons Task Force on Automation and Reproducibility](../taskforce.md),
[cTuning.org](https://cTuning.org) and [cKnowledge.org](https://cKnowledge.org)
led by [Grigori Fursin](https://cKnowledge.org/gfursin) and [Arjun Suresh](https://www.linkedin.com/in/arjunsuresh).
You can learn more about our plans and long-term vision from our [ACM REP'23 keynote](https://doi.org/10.5281/zenodo.8105339).

Don't hesitate to get in touch with the CM/CK community via our [public Discord server](https://discord.gg/JjWNWXKxwT)
to provide your feedback, ask questions, add new benchmark implementations, models, data sets and hardware backends,
prepare and optimize your MLPerf submissions and participate in our [reproducibility and optimization challenges](https://access.cknowledge.org/playground/?action=challenges).
