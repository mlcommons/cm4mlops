# Reproducibility report: system setup

**Tags:** MLPerf inference v1.1; Image Classification; Resnet50; ONNX (out of the box); GCP n2-standard-80; X64; datacenter; open division

## Install system dependencies

* https://github.com/mlcommons/ck/blob/master/docs/mlperf-automation/platform/x8664-ubuntu.md

## Check python version (3.7+)

```bash
python3 --version

> 3.7.10
```

## Install [CK automation framework](https://github.com/mlcommons/ck)

Tested with the version 2.5.8

```bash
python3 -m pip install ck -U
```

*You may need to restart your bash to initialize PATH to CK scripts.*

## Prepare virtual environment with common MLPerf components

*You can either install stable components for MLPerf v1.1 (can't be updated) 
or the latest development components that you can update at any time:*

### Stable components for MLPerf v1.1

```bash
ck pull repo:mlcommons@ck-venv --checkout=mlperf-1.1-mlcommons
ck create venv:mlperf-inference --template=mlperf-inference-1.1-tvm-mlcommons
```

See CK packages that will be automatically installed from the [mlperf-inference-1.1-tvm-mlcommons template](https://github.com/mlcommons/ck-venv/blob/master/venv.template/mlperf-inference-1.1-tvm-mlcommons/script.sh).

Find more about CK automation for virtual environments [here](https://github.com/mlcommons/ck-venv).

### Development (live) components for MLPerf

```bash
ck pull repo:mlcommons@ck-venv
ck create venv:mlperf-inference9 --template=mlperf-inference-1.1-tvm
```

See CK packages that will be automatically installed from the [mlperf-inference-1.1-tvm template](https://github.com/mlcommons/ck-venv/blob/master/venv.template/mlperf-inference-1.1-tvm/script.sh).


## Activate virtual environment

```bash
ck activate venv:mlperf-inference
```

## Check installed CK packages

```bash
ck show env
```

## Install CK repository with ML/MLPerf automations 

```bash
ck where repo:mlcommons@ck-mlops
ck pull repo:ck-mlperf-inference
```

## Install CK components

```bash
ck install package --tags=lib,python-package,onnxruntime-cpu,1.8.1
ck install package --tags=lib,python-package,onnx,1.10.1

```

## Prepare image classification task

Install ImageNet aux via CK:

```bash
ck install package --tags=imagenet,2012,aux
```

### Plug local ImageNet into CK

Note that the ImageNet validation dataset is not available for public download. 
Please follow these [instructions](https://github.com/mlcommons/ck/blob/master/docs/mlperf-automation/datasets/imagenet2012.md) to obtain it.

Find the path with the ImageNet validation set (50,000 images) and plug it into CK as follows:

```bash
ck detect soft:dataset.imagenet.val --force_version=2012 --extra_tags=full \
      --search_dir={directory where the ImageNet val dataset is downloaded}
```

Alternatively, you can install reduced ImageNet (500 images) via CK just to test your installation and CK workflows:
```bash
ck install package --tags=imagenet,2012,val,min,non-resized
```

### See all installed packages and detected components

```bash
ck show env
ck show env --tags=mlcommons,src
```



# Reproducibility report: benchmarking

First, you must prepare your system based on the "Reproducibility report: system setup".


## Install ONNX model (ResNet50; opset-11; FP32)

Install MLPerf model:

```bash
ck install package --tags=model,image-classification,mlperf,onnx,v1.5-opset-11
```

More information about this model: 
[ [CK meta.json](https://github.com/mlcommons/ck-mlops/blob/main/package/ml-model-mlperf-resnet50-onnx/.cm/meta.json) ]


## Install CK workflow Python dependencies

```bash
ck run program:mlperf-inference-bench-image-classification-onnx-cpu \
     --cmd_key=install-python-requirements
```

## Run Offline scenario

### Accuracy

```bash
time ck benchmark program:mlperf-inference-bench-image-classification-onnx-cpu \
     --repetitions=1 --skip_print_timers --skip_print_stats --skip_stat_analysis \
     --cmd_key=accuracy-offline \
     --env.EXTRA_OPS="" \
     --print_files=accuracy.txt
```

### Performance

```bash

time ck benchmark program:mlperf-inference-bench-image-classification-onnx-cpu \
     --repetitions=1 --skip_print_timers --skip_print_stats --skip_stat_analysis \
     --cmd_key=performance-offline \
     --env.EXTRA_OPS="--qps 150 --time 610" \
     --print_files=mlperf_log_summary.txt

```





## Prepare your submission

One can use the [end-to-end MLPerf submission and visualization workflow](https://github.com/mlcommons/ck-mlops/tree/main/module/bench.mlperf.inference)
(collaboration between MLCommons, OctoML and the cTuning foundation) to prepare the above submission as follows:

```bash

ck activate venv:mlperf-inference

ck pull repo:ck-mlperf-inference

ck install package --tags=mlperf,inference,results,dummy

ck set kernel --var.mlperf_inference_version=1.1
ck set kernel --var.mlperf_inference_submitter=cTuning
ck set kernel --var.mlperf_inference_division=open

ck add bench.mlperf.system:gcp-n2-standard-80-onnx-out-of-the-box --base=1-node-2s-clx-tf-int8
ck set kernel --var.mlperf_inference_system=gcp-n2-standard-80-onnx-out-of-the-box

ck run bench.mlperf.inference  --framework=onnx --model=resnet50 --scenario=offline --mode=prereq

ck run bench.mlperf.inference  --framework=onnx --model=resnet50 --scenario=offline --mode=accuracy \
     --skip_system_ext \
     --env.EXTRA_OPS=""


ck run bench.mlperf.inference  --framework=onnx --model=resnet50 --scenario=offline --mode=performance \
     --clean \
     --skip_system_ext \
     --env.EXTRA_OPS="--qps 400 --time 610"
```

You should truncate your accuracy files before submitting results:
```bash
ck run program:mlperf-inference-submission --cmd_key=truncate_accuracy_log --env.CK_MLPERF_SUBMITTER=cTuning
```

Check the submission by the MLPerf submission checker:
```bash
ck run program:mlperf-inference-submission --cmd_key=check
```

Pack results:
```
ck zip bench.mlperf.system
```



## Visualize MLPerf results

* [Prototype of a local dashboard for the MLPerf inference benchmark](https://github.com/mlcommons/ck-mlops/blob/main/module/bench.mlperf.inference/README.results.md)





## Resources

Our on-going collaboration with MLCommons to make 
the MLPerf&trade; inference benchmark more customizable, portable and easy to use:

* [Motivation](https://www.youtube.com/watch?v=7zpeIVwICa4)
* [MLCommons working groups](https://mlcommons.org/en/groups)
* [Open-source CK framework](https://github.com/mlcommons/ck) and [MLCube](https://github.com/mlcommons/mlcube)
  * [ML/AI packages from the community, OctoML and the cTuning foundation](https://github.com/mlcommons/ck-mlops/tree/main/package)
  * [ML/AI workflows from the community, OctoML and the cTuning foundation](https://github.com/mlcommons/ck-mlops/tree/main/program)
* [Documentation for the CK-powered MLPerf automation suite](https://github.com/mlcommons/ck/tree/master/docs/mlperf-automation)
* [Prototype of the end-to-end submission workflow for the MLPerf inference benchmark](https://github.com/mlcommons/ck-mlops/tree/main/module/bench.mlperf.inference)
