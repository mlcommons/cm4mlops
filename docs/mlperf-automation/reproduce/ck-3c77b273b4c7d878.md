# Reproducibility report: system setup

**Tags:** MLPerf inference v1.1; Image Classification; Resnet50; TVM; Raspberry Pi 4; ARM64; edge; closed division

## Install system dependencies

* https://github.com/mlcommons/ck/blob/master/docs/mlperf-automation/platform/rpi4-ubuntu.md

*Make sure that you have a large swap disk, proper power supply and cooling to ensure stable MLPerf results.*

## Check python version (3.7+)

```bash
python3 --version

> 3.7.10
```

## Install [CK automation framework](https://github.com/mlcommons/ck)

Tested with the version 2.5.8

```bash
python3 -m pip install ck -U
```

*You may need to restart your bash to initialize PATH to CK scripts.*

## Prepare virtual environment with common MLPerf components

```bash
ck pull repo:mlcommons@ck-venv --checkout=mlperf-1.1-mlcommons
ck create venv:mlperf-inference --template=mlperf-inference-1.1-tvm-mlcommons
```

See CK packages that will be automatically installed from the [mlperf-inference-1.1-tvm-mlcommons template](https://github.com/mlcommons/ck-venv/blob/master/venv.template/mlperf-inference-1.1-tvm-mlcommons/script.sh).

Find more about CK automation for virtual environments [here](https://github.com/mlcommons/ck-venv).

## Activate virtual environment

```bash
ck activate venv:mlperf-inference
```

## Check installed CK packages

```bash
ck show env
```

## Install CK repository with ML/MLPerf automations 

```bash
ck where repo:mlcommons@ck-mlops
ck pull repo:ck-mlperf-inference
```

## Install CK components

```bash
ck install package --tags=lib,python-package,onnxruntime-cpu,1.8.1
ck install package --tags=lib,python-package,onnx,1.10.1

ck install package --tags=lib,python-package,pytorch,cpu,1.9.0
ck install package --tags=lib,python-package,torchvision,cpu,0.10.0

ck install package --tags=tool,cmake,prebuilt,v3.21.1
ck install package --tags=compiler,llvm,prebuilt,v12.0.0

ck install package --tags=compiler,tvm,src,dev-0.8-dnnl-int8-v2-mlperf-1.1 \
      --env.USE_OPENMP=gnu --j=2 --quiet
```

## Prepare image classification task

```
ck install package --tags=imagenet,2012,aux
```

### Plug local ImageNet into CK

Note that the ImageNet validation dataset is not available for public download. 
Please follow these [instructions](https://github.com/mlcommons/ck/blob/master/docs/mlperf-automation/datasets/imagenet2012.md) to obtain it.

Find the path with the ImageNet validation set (50,000 images) and plug it into CK as follows:

```bash
ck detect soft:dataset.imagenet.val --force_version=2012 --extra_tags=full \
      --search_dir={directory where the ImageNet val dataset is downloaded}
```

### See all installed packages and detected components

```bash
ck show env
ck show env --tags=tvm
ck show env --tags=mlcommons,src
ck locate env --tags=tvm
```




# Reproducibility report: benchmarking

First, you must prepare your system based on the "Reproducibility report: system setup".


## Install ONNX model (ResNet50; opset-11; FP32)

Install MLPerf model:

```bash
ck install package --tags=model,image-classification,mlperf,onnx,v1.5-opset-11
```

More information about this model: 
[ [CK meta.json](https://github.com/mlcommons/ck-mlops/blob/main/package/ml-model-mlperf-resnet50-onnx/.cm/meta.json) ]


## Install CK workflow Python dependencies

```bash
ck run program:mlperf-inference-bench-image-classification-tvm-onnx-cpu \
     --cmd_key=install-python-requirements
```


## Run Offline scenario

Since it's too long to run this scenario and we can't satisfy min number of samples (24K), we use SingleStream results.

You can still try to run offline mode as follows:

### Accuracy

```bash
time ck benchmark program:mlperf-inference-bench-image-classification-tvm-onnx-cpu \
     --repetitions=1 --skip_print_timers --skip_print_stats --skip_stat_analysis \
     --cmd_key=accuracy-offline \
     --env.MLPERF_BACKEND=tvm \
     --env.MLPERF_TVM_EXECUTOR=graph \
     --env.MLPERF_TVM_TARGET="llvm" \
     --env.OMP_NUM_THREADS=1 \
     --env.TVM_NUM_THREADS=1 \
     --env.EXTRA_OPS="--max-batchsize 1" \
     --print_files=accuracy.txt
```

Extra customization:
* ```--env.EXTRA_OPS="--threads 1 --max-batchsize 1 --count 100 --time 60")```
* ```llvm -mcpu=cortex-a72 -mfloat-abi=hard```


### Performance

```bash
time ck benchmark program:mlperf-inference-bench-image-classification-tvm-onnx-cpu \
     --repetitions=1 --skip_print_timers --skip_print_stats --skip_stat_analysis \
     --dep_add_tags.dataset=full \
     --cmd_key=performance-offline \
     --env.MLPERF_BACKEND=tvm \
     --env.MLPERF_TVM_EXECUTOR=graph \
     --env.MLPERF_TVM_TARGET="llvm" \
     --env.OMP_NUM_THREADS=1 \
     --env.TVM_NUM_THREADS=1 \
     --env.EXTRA_OPS="--max-batchsize 1 --qps 2 --count 1024" \
     --print_files=mlperf_log_summary.txt --no_clean
```

## SingleStream scenario

### Accuracy

```bash
time ck benchmark program:mlperf-inference-bench-image-classification-tvm-onnx-cpu \
     --repetitions=1 --skip_print_timers --skip_print_stats --skip_stat_analysis \
     --cmd_key=accuracy-singlestream \
     --env.MLPERF_BACKEND=tvm \
     --env.MLPERF_TVM_EXECUTOR=graph \
     --env.MLPERF_TVM_TARGET="llvm" \
     --env.OMP_NUM_THREADS=4 \
     --env.TVM_NUM_THREADS=4 \
     --env.EXTRA_OPS="--threads 1 --max-batchsize 1" \
     --print_files=accuracy.txt
```

### Performance

```bash
time ck benchmark program:mlperf-inference-bench-image-classification-tvm-onnx-cpu \
     --no_clean \
     --repetitions=1 --skip_print_timers --skip_print_stats --skip_stat_analysis \
     --cmd_key=performance-singlestream \
     --env.MLPERF_BACKEND=tvm \
     --env.MLPERF_TVM_EXECUTOR=graph \
     --env.MLPERF_TVM_TARGET="llvm" \
     --env.OMP_NUM_THREADS=4 \
     --env.TVM_NUM_THREADS=4 \
     --env.EXTRA_OPS="--threads 1 --max-batchsize 1 --count 2048" \
     --print_files=mlperf_log_summary.txt
```


*Add "--no_clean" flag to avoid recompiling TVM model*





## Prepare your submission

One can use the [end-to-end MLPerf submission and visualization workflow](https://github.com/mlcommons/ck-mlops/tree/main/module/bench.mlperf.inference)
(collaboration between MLCommons, OctoML and the cTuning foundation) to prepare the above submission as follows:

```bash

ck activate venv:mlperf-inference

ck pull repo:ck-mlperf-inference

ck install package --tags=mlperf,inference,results,dummy

ck set kernel --var.mlperf_inference_version=1.1
ck set kernel --var.mlperf_inference_submitter=cTuning
ck set kernel --var.mlperf_inference_division=closed

ck add bench.mlperf.system:rpi4-tvm-fp32 --base=rpi4-tflite-v2.2.0-ruy
ck set kernel --var.mlperf_inference_system=rpi4-tvm-fp32

ck run bench.mlperf.inference  --framework=tvm-pytorch --model=resnet50 --scenario=offline --mode=prereq

ck run bench.mlperf.inference  --framework=tvm-onnx --model=resnet50 --scenario=singlestream --mode=accuracy \
        --clean \
        --skip_system_ext \
        --duplicate_to_offline \
        --env.MLPERF_BACKEND=tvm \
        --env.MLPERF_TVM_EXECUTOR=graph \
        --env.MLPERF_TVM_TARGET="llvm" \
        --env.OMP_NUM_THREADS=4 \
        --env.TVM_NUM_THREADS=4 \
        --env.EXTRA_OPS="--threads 1 --max-batchsize 1"

ck run bench.mlperf.inference  --framework=tvm-onnx --model=resnet50 --scenario=singlestream --mode=performance \
        --skip_system_ext \
        --duplicate_to_offline \
        --compliance --quiet \
        --env.MLPERF_BACKEND=tvm \
        --env.MLPERF_TVM_EXECUTOR=graph \
        --env.MLPERF_TVM_TARGET="llvm" \
        --env.OMP_NUM_THREADS=4 \
        --env.TVM_NUM_THREADS=4 \
        --env.EXTRA_OPS="--threads 1 --max-batchsize 1 --count 2048"
```

You should truncate your accuracy files before submitting results:
```bash
ck run program:mlperf-inference-submission --cmd_key=truncate_accuracy_log --env.CK_MLPERF_SUBMITTER=cTuning
```

Check the submission by the MLPerf submission checker:
```bash
ck run program:mlperf-inference-submission --cmd_key=check
```

Pack results:
```
ck zip bench.mlperf.system
```



## Visualize MLPerf results

* [Prototype of a local dashboard for the MLPerf inference benchmark](https://github.com/mlcommons/ck-mlops/blob/main/module/bench.mlperf.inference/README.results.md)





## Resources

Our on-going collaboration with MLCommons to make 
the MLPerf&trade; inference benchmark more customizable, portable and easy to use:

* [Motivation](https://www.youtube.com/watch?v=7zpeIVwICa4)
* [MLCommons working groups](https://mlcommons.org/en/groups)
* [Open-source CK framework](https://github.com/mlcommons/ck) and [MLCube](https://github.com/mlcommons/mlcube)
  * [ML/AI packages from the community, OctoML and the cTuning foundation](https://github.com/mlcommons/ck-mlops/tree/main/package)
  * [ML/AI workflows from the community, OctoML and the cTuning foundation](https://github.com/mlcommons/ck-mlops/tree/main/program)
* [Documentation for the CK-powered MLPerf automation suite](https://github.com/mlcommons/ck/tree/master/docs/mlperf-automation)
* [Prototype of the end-to-end submission workflow for the MLPerf inference benchmark](https://github.com/mlcommons/ck-mlops/tree/main/module/bench.mlperf.inference)
