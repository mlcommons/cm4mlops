# Identification of this CM script
alias: app-mlperf-inference-nvidia
uid: 689e865b0059479b

automation_alias: script
automation_uid: 5b4e0237da074764

category: "Modular MLPerf inference benchmark pipeline"


# User-friendly tags to find this CM script
tags:
  - app
  - mlcommons
  - mlperf
  - inference
  - nvidia-harness
  - nvidia

# Default environment
default_env:
  CM_BATCH_COUNT: '1'
  CM_BATCH_SIZE: '1'
  CM_FAST_COMPILATION: "yes"
  CM_MLPERF_LOADGEN_SCENARIO: Offline

# Map script inputs to environment variables
input_mapping:
  count: CM_MLPERF_LOADGEN_QUERY_COUNT
  max_batchsize: CM_MLPERF_LOADGEN_MAX_BATCHSIZE
  mlperf_conf: CM_MLPERF_CONF
  mode: CM_MLPERF_LOADGEN_MODE
  output_dir: CM_MLPERF_OUTPUT_DIR
  performance_sample_count: CM_MLPERF_LOADGEN_PERFORMANCE_SAMPLE_COUNT
  scenario: CM_MLPERF_LOADGEN_SCENARIO
  user_conf: CM_MLPERF_USER_CONF

new_env_keys:
  - CM_MLPERF_*
  - CM_DATASET_*
  - CM_HW_NAME

# Dependencies on other CM scripts

deps:

  # Detect host OS features
  - tags: detect,os

  # Detect host CPU features
  - tags: detect,cpu

  # Install system dependencies on a given host
  - tags: get,sys-utils-cm

  # Detect CUDA
  - tags: get,cuda,_cudnn

  # Detect Tensorrt
  - tags: get,tensorrt

  # Detect Google Logger
  - tags: get,generic,sys-util,_glog-dev

  # Detect GFlags
  - tags: get,generic,sys-util,_gflags-dev

  ########################################################################
  # Install MLPerf inference dependencies
  
  # Install MLPerf loadgen
  - tags: get,loadgen
    names:
    - loadgen
    
  # Download MLPerf inference source
  - tags: get,mlcommons,inference,src
    names:
    - inference-src

  # Download Nvidia Submission Code
  - tags: get,nvidia,mlperf,inference,common-code
    names:
    - nvidia-inference-common-code


  ########################################################################
  # Install ResNet50 model (ONNX) and ImageNet

  - enable_if_env:
      CM_MODEL:
      - resnet50
    names:
      - imagenet-preprocessed
    tags: get,dataset,preprocessed,imagenet,_NCHW

  - enable_if_env:
      CM_MODEL:
      - resnet50
    names:
      - ml-model
      - resnet50-model
    tags: get,ml-model,resnet50,_onnx


  ########################################################################
  # Install RetinaNet model (ONNX) and OpenImages

  - enable_if_env:
      CM_MODEL:
      - retinanet
    names:
      - openimages-preprocessed
    tags: get,dataset,preprocessed,openimages,_validation,_NCHW

  - enable_if_env:
      CM_MODEL:
      - retinanet
    names:
      - ml-model
      - retinanet-model
    tags: get,ml-model,retinanet,_onnx,_fp32
  - skip_if_env:
      CM_MLPERF_DEVICE:
      -  cpu
    tags: generate,nvidia,engine
    names: tensorrt-engine-generator

  # Creates user conf for given SUT
  - tags: generate,user-conf,mlperf,inference
    names:
    - user-conf-generator

  

# Post dependencies to compile and run this app
post_deps:

  - names:
    - compile-program
    tags: compile,cpp-program

  - names:
    - mlperf-runner
    tags: benchmark-mlperf


# Variations to customize dependencies
variations:
  # Target devices
  cpu:
    group: device
    default: true
    env:
      CM_MLPERF_DEVICE: cpu
  cuda:
    env:
      CM_MLPERF_DEVICE: gpu
      CM_MLPERF_DEVICE_LIB_NAMESPEC: cudart

  pytorch:
    group: framework
    env:
      CM_MLPERF_BACKEND: pytorch


  # Reference MLPerf models
  resnet50:
    group: model
    default: true
    env:
      CM_MODEL: resnet50

  retinanet:
    group: model
    env:
      CM_MODEL: retinanet

  batch_size.#:
    env:
      CM_MODEL_BATCH_SIZE: #
