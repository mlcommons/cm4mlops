# Identification of this CM script
alias: build-mlperf-inference-server-nvidia
uid: f37403af5e9f4541
cache: true
automation_alias: script
automation_uid: 5b4e0237da074764
default_version: r3.0
category: "Modular MLPerf benchmarks"


# User-friendly tags to find this CM script
tags:
  - build
  - mlcommons
  - mlperf
  - inference
  - inference-server
  - server
  - nvidia-harness
  - nvidia


new_env_keys:
  - CM_MLPERF_INFERENCE_NVIDIA_CODE_PATH

default_env:
  CM_MAKE_BUILD_COMMAND: build
  CM_MAKE_CLEAN: "no"

input_mapping:
  custom_system: CM_CUSTOM_SYSTEM_NVIDIA
  clean: CM_MAKE_CLEAN

# Dependencies on other CM scripts

deps:

  # Detect host OS features
  - tags: detect,os

  # Detect host CPU features
  - tags: detect,cpu

  # Install system dependencies on a given host
  - tags: get,sys-utils-cm

  # Detect python3
  - tags: get,python3
    names:
    - python
    - python3

  # Detect CUDA
  - tags: get,cuda,_cudnn
    names:
    - cuda
    enable_if_env:
      CM_MLPERF_DEVICE:
        - cuda
        - inferentia

  # Detect Tensorrt
  - tags: get,tensorrt,_dev
    names:
      - tensorrt
    enable_if_env:
      CM_MLPERF_DEVICE:
        - cuda
        - inferentia

  # Detect gcc
  - tags: get,gcc

  # Detect CMake
  - tags: get,cmake
    version_min: "3.18"

  # Detect Google Logger
  - tags: get,generic,sys-util,_glog-dev

  # Detect GFlags
  - tags: get,generic,sys-util,_gflags-dev

  # Detect libgmock-dev
  - tags: get,generic,sys-util,_libgmock-dev

  # Detect libre2-dev
  - tags: get,generic,sys-util,_libre2-dev

  # Detect libnuma-dev
  - tags: get,generic,sys-util,_libnuma-dev

  # Detect libboost-all-dev
  - tags: get,generic,sys-util,_libboost-all-dev

  # Detect rapidjson-dev
  - tags: get,generic,sys-util,_rapidjson-dev


  # Download Nvidia Submission Code
  - tags: get,nvidia,mlperf,inference,common-code
    names:
    - nvidia-inference-common-code

  # Detect pycuda
  - tags: get,generic-python-lib,_pycuda
    skip_if_env:
      CM_RUN_STATE_DOCKER:
        - 'yes'
        - True
        - 'True'

  # Detect opencv-python
  - tags: get,generic-python-lib,_opencv-python

  # Detect nvidia-dali
  - tags: get,generic-python-lib,_nvidia-dali

  # Get Nvidia scratch space where data and models get downloaded
  - tags: get,mlperf,inference,nvidia,scratch,space
    names:
    - nvidia-scratch-space

post_deps:
  # Detect nvidia system
  - tags: add,custom,system,nvidia
    names:
    - custom-system-nvidia
    - nvidia-inference-common-code
    skip_if_env:
      CM_CUSTOM_SYSTEM_NVIDIA:
      - "no"
      - False
      - "False"



variations:
  # Target devices
  cpu:
    group: device
    env:
      CM_MLPERF_DEVICE: cpu
  inferentia:
    group: device
    env:
      CM_MLPERF_DEVICE: inferentia
  cuda:
    group: device
    default: true
    env:
      CM_MLPERF_DEVICE: cuda
      CM_MLPERF_DEVICE_LIB_NAMESPEC: cudart

  nvidia-only:
    group: code
    default: true
    add_deps_recursive:
      nvidia-inference-common-code:
        tags: _nvidia-only
  custom:
    group: code
    add_deps_recursive:
      nvidia-inference-common-code:
        tags: _custom
  mlcommons:
    group: code
    add_deps_recursive:
      nvidia-inference-common-code:
        tags: _mlcommons


versions:
  r2.1:
    add_deps_recursive:
      nvidia-inference-common-code:
        version: r2.1

  r3.0:
    add_deps_recursive:
      nvidia-inference-common-code:
        version: r3.0
  r3.1:
    add_deps_recursive:
      nvidia-inference-common-code:
        version: r3.1

docker:
  skip_run_cmd: 'no'
  all_gpus: 'yes'
  docker_input_mapping:
    imagenet_path: IMAGENET_PATH
    results_dir: RESULTS_DIR
    submission_dir: SUBMISSION_DIR
    cudnn_tar_file_path: CM_CUDNN_TAR_FILE_PATH
    tensorrt_tar_file_path: CM_TENSORRT_TAR_FILE_PATH
    cuda_run_file_path: CUDA_RUN_FILE_LOCAL_PATH
    scratch_path: MLPERF_SCRATCH_PATH
  mounts:
   - "${{ IMAGENET_PATH }}:${{ IMAGENET_PATH }}"
   - "${{ RESULTS_DIR }}:/home/cmuser/results_dir"
   - "${{ SUBMISSION_DIR }}:/home/cmuser/submission_dir"
   - "${{ CM_CUDNN_TAR_FILE_PATH }}:${{ CM_CUDNN_TAR_FILE_PATH }}"
   - "${{ CM_TENSORRT_TAR_FILE_PATH }}:${{ CM_TENSORRT_TAR_FILE_PATH }}"
   - "${{ CUDA_RUN_FILE_LOCAL_PATH }}:${{ CUDA_RUN_FILE_LOCAL_PATH }}"
   - "${{ MLPERF_SCRATCH_PATH }}:${{ MLPERF_SCRATCH_PATH }}"
