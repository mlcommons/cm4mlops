# Identification of this CM script
alias: reproduce-mlperf-inference-nvidia
uid: bc3b17fb430f4732
cache: false
can_force_cache: true

automation_alias: script
automation_uid: 5b4e0237da074764

category: "Modular MLPerf benchmarks"


# User-friendly tags to find this CM script
tags:
  - reproduce
  - mlcommons
  - mlperf
  - inference
  - harness
  - nvidia-harness
  - nvidia

# Default environment
default_env:
  CM_BATCH_COUNT: '1'
  CM_BATCH_SIZE: '1'
  CM_FAST_COMPILATION: 'yes'
  CM_MLPERF_LOADGEN_SCENARIO: Offline
  CM_MLPERF_LOADGEN_MODE: performance
  SKIP_POLICIES: '1'
  CM_SKIP_PREPROCESS_DATASET: 'no'
  CM_SKIP_MODEL_DOWNLOAD: 'no'
  CM_MLPERF_SUT_NAME_IMPLEMENTATION_PREFIX: nvidia_original
  CM_CALL_RUNNER: 'yes'

# Map script inputs to environment variables
input_mapping:
  count: CM_MLPERF_LOADGEN_QUERY_COUNT
  max_batchsize: CM_MLPERF_LOADGEN_MAX_BATCHSIZE
  mlperf_conf: CM_MLPERF_CONF
  mode: CM_MLPERF_LOADGEN_MODE
  output_dir: CM_MLPERF_OUTPUT_DIR
  performance_sample_count: CM_MLPERF_LOADGEN_PERFORMANCE_SAMPLE_COUNT
  scenario: CM_MLPERF_LOADGEN_SCENARIO
  user_conf: CM_MLPERF_USER_CONF
  devices: CM_MLPERF_NVIDIA_HARNESS_DEVICES
  skip_preprocess: CM_SKIP_PREPROCESS_DATASET
  skip_preprocessing: CM_SKIP_PREPROCESS_DATASET
  target_qps: CM_MLPERF_LOADGEN_TARGET_QPS
  offline_target_qps: CM_MLPERF_LOADGEN_OFFLINE_TARGET_QPS
  server_target_qps: CM_MLPERF_LOADGEN_SERVER_TARGET_QPS
  target_latency: CM_MLPERF_LOADGEN_TARGET_LATENCY
  singlestream_target_latency: CM_MLPERF_LOADGEN_SINGLESTREAM_TARGET_LATENCY
  multistream_target_latency: CM_MLPERF_LOADGEN_MULTISTREAM_TARGET_LATENCY
  use_triton: CM_MLPERF_NVIDIA_HARNESS_USE_TRITON
  gpu_copy_streams: CM_MLPERF_NVIDIA_HARNESS_GPU_COPY_STREAMS
  gpu_inference_streams: CM_MLPERF_NVIDIA_HARNESS_GPU_INFERENCE_STREAMS
  gpu_batch_size: CM_MLPERF_NVIDIA_HARNESS_GPU_BATCH_SIZE
  dla_copy_streams: CM_MLPERF_NVIDIA_HARNESS_DLA_COPY_STREAMS
  dla_inference_streams: CM_MLPERF_NVIDIA_HARNESS_DLA_INFERENCE_STREAMS
  dla_batch_size: CM_MLPERF_NVIDIA_HARNESS_DLA_BATCH_SIZE
  input_format: CM_MLPERF_NVIDIA_HARNESS_INPUT_FORMAT
  performance_sample_count: CM_MLPERF_LOADGEN_PERFORMANCE_SAMPLE_COUNT
  workspace_size: CM_MLPERF_NVIDIA_HARNESS_WORKSPACE_SIZE
  log_dir: CM_MLPERF_NVIDIA_HARNESS_LOG_DIR
  use_graphs: CM_MLPERF_NVIDIA_HARNESS_USE_GRAPHS
  run_infer_on_copy_streams: CM_MLPERF_NVIDIA_HARNESS_RUN_INFER_ON_COPY_STREAMS
  start_from_device: CM_MLPERF_NVIDIA_HARNESS_START_FROM_DEVICE
  end_on_device: CM_MLPERF_NVIDIA_HARNESS_END_ON_DEVICE
  max_dlas: CM_MLPERF_NVIDIA_HARNESS_MAX_DLAS
  power_setting: CM_MLPERF_NVIDIA_HARNESS_POWER_SETTING
  make_cmd: MLPERF_NVIDIA_RUN_COMMAND
  rerun: CM_RERUN

new_state_keys:
  - mlperf-inference-implementation
  - CM_SUT_*

# Dependencies on other CM scripts

deps:

  # Detect host OS features
  - tags: detect,os

  # Detect host CPU features
  - tags: detect,cpu

  # Install system dependencies on a given host
  - tags: get,sys-utils-cm

  # Detect CUDA
  - names:
    - cuda
    tags: get,cuda,_cudnn

  # Detect Tensorrt
  - names:
    - tensorrt
    tags: get,tensorrt

  # Build nvidia inference server
  - names:
    - nvidia-inference-server
    tags: build,nvidia,inference,server

  # Get Nvidia scratch space where data and models get downloaded
  - tags: get,mlperf,inference,nvidia,scratch,space
    names:
    - nvidia-scratch-space

  # Get MLPerf logging library
  - tags: get,generic-python-lib,_mlperf_logging
    names:
    - mlperf-logging

  ########################################################################
  # Install ResNet50 model (ONNX) and ImageNet

  - enable_if_env:
      CM_MODEL:
      - resnet50
    names:
      - imagenet-original
    tags: get,dataset,original,imagenet,_full

  - enable_if_env:
      CM_MODEL:
      - resnet50
    names:
      - resnet50-model
      - ml-model
    tags: get,ml-model,resnet50,_fp32,_onnx,_opset-8

  ########################################################################
  # Install kits19 dataset

  - enable_if_env:
      CM_MODEL:
      - 3d-unet-99
      - 3d-unet-99.9
    names:
      - kits19-original
    tags: get,dataset,original,kits19


  ########################################################################
  # Install librispeech dataset

  - enable_if_env:
      CM_MODEL:
      - rnnt
    names:
      - librispeech-original
    tags: get,dataset,original,librispeech

  ########################################################################
  # Install criteo dataset

  - enable_if_env:
      CM_MODEL:
      - dlrm-99
      - dlrm-99.9
    names:
      - criteo-original
    tags: get,dataset,original,criteo

  ########################################################################
  # Install dlrm model
  - enable_if_env:
      CM_MODEL:
      - dlrm-99
      - dlrm-99.9
    names:
      - dlrm-model
    tags: get,ml-model,dlrm,_pytorch

  ########################################################################
  # Install bert models
  - enable_if_env:
      CM_MODEL:
      - bert-99
      - bert-99.9
    names:
      - bert-model
      - bert-model-fp32
    tags: get,ml-model,bert,_onnx,_fp32

  - enable_if_env:
      CM_MODEL:
      - bert-99
      - bert-99.9
    names:
      - bert-model
      - bert-model-int8
    tags: get,ml-model,bert,_onnx,_int8

  - enable_if_env:
      CM_MODEL:
      - bert-99
      - bert-99.9
    names:
      - bert-vocab
    tags: get,squad-vocab

  ########################################################################
  # Install OpenImages

  - enable_if_env:
      CM_MODEL:
      - retinanet
    names:
      - openimages-original
    tags: get,dataset,original,openimages,_validation,_full,_custom-annotations

  - enable_if_env:
      CM_MODEL:
      - retinanet
    names:
      - openimages-calibration
    tags: get,dataset,original,openimages,_calibration



  ########################################################################
  # Install MLPerf inference dependencies

  # Download MLPerf inference source
  - tags: get,mlcommons,inference,src
    names:
    - inference-src

  # Download Nvidia Submission Code
  - tags: get,nvidia,mlperf,inference,common-code
    names:
    - nvidia-inference-common-code

  # Creates user conf for given SUT
  - tags: generate,user-conf,mlperf,inference
    names:
    - user-conf-generator
    enable_if_env:
      CM_MLPERF_NVIDIA_HARNESS_RUN_MODE:
      - run_harness


# Post dependencies to run this app including for power measurement
post_deps:

  - names:
    - runner
    skip_if_env:
      CM_MLPERF_SKIP_RUN:
        - yes
      CM_MLPERF_NVIDIA_HARNESS_RUN_MODE:
        - run_harness
    tags: benchmark-mlperf
    enable_if_env:
      CM_CALL_RUNNER:
        - yes


# Variations to customize dependencies
variations:
  # Target devices
  cpu:
    group: device
    env:
      CM_MLPERF_DEVICE: cpu
  cuda:
    group: device
    default: true
    env:
      CM_MLPERF_DEVICE: gpu
      CM_MLPERF_DEVICE_LIB_NAMESPEC: cudart

  tensorrt:
    group: backend
    default: true
    env:
      CM_MLPERF_BACKEND: tensorrt
      CM_MLPERF_BACKEND_NAME: TensorRT

  # Reference MLPerf models
  resnet50:
    group: model
    default: true
    env:
      CM_MODEL: resnet50

  retinanet:
    group: model
    env:
      CM_MODEL: retinanet
    deps:
    - tags: get,generic-python-lib,_Pillow
    - tags: get,generic-python-lib,_torch
    - tags: get,generic-python-lib,_torchvision
    - tags: get,generic-python-lib,_opencv-python

  bert_:
    deps:
    - tags: get,generic-python-lib,_transformers
    - tags: get,generic-python-lib,_onnx

  bert-99:
    group: model
    base:
    - bert_
    env:
      CM_MODEL: bert-99

  bert-99.9:
    group: model
    base:
    - bert_
    env:
      CM_MODEL: bert-99.9

  3d-unet_:
    deps:
    - tags: get,generic-python-lib,_transformers

  3d-unet-99:
    group: model
    base:
    - 3d-unet_
    env:
      CM_MODEL: 3d-unet-99

  3d-unet-99.9:
    group: model
    base:
    - 3d-unet_
    env:
      CM_MODEL: 3d-unet-99.9

  rnnt:
    group: model
    env:
      CM_MODEL: rnnt
    deps:
    - tags: get,generic-python-lib,_toml
    - tags: get,generic-python-lib,_torchvision
      names:
      - torchvision
    - tags: get,generic-python-lib,_torch
    - tags: get,generic-python-lib,_nvidia-apex
    - tags: get,generic-python-lib,_unidecode
    - tags: get,generic-python-lib,_inflect
    - tags: get,generic-python-lib,_librosa
      names:
        - librosa
    - tags: get,generic-python-lib,_sox
    - tags: get,generic-sys-util,_sox

  dlrm_:
    deps:
    - tags: get,generic-python-lib,_torch

  dlrm-99:
    group: model
    base:
    - dlrm_
    env:
      CM_MODEL: dlrm-99

  dlrm-99.9:
    group: model
    base:
    - dlrm_
    env:
      CM_MODEL: dlrm-99.9

  batch_size.#:
    group: batch-size
    env:
      CM_MODEL_BATCH_SIZE: "#"
      CM_MLPERF_NVIDIA_HARNESS_GPU_BATCH_SIZE: "#"
      #CM_MLPERF_SUT_NAME_RUN_CONFIG_SUFFIX1: "gpu_batch_size.#"

  dla_batch_size.#:
    env:
      CM_MLPERF_NVIDIA_HARNESS_DLA_BATCH_SIZE: "#"
      CM_MLPERF_SUT_NAME_RUN_CONFIG_SUFFIX2: "dla_batch_size.#"

  use_triton:
    env:
      CM_MLPERF_NVIDIA_HARNESS_USE_TRITON: "yes"
      CM_MLPERF_SUT_NAME_RUN_CONFIG_SUFFIX3: "using_triton"

  preprocess_data:
    group: run-mode
    env:
      MLPERF_NVIDIA_RUN_COMMAND: preprocess_data
      CM_MLPERF_NVIDIA_HARNESS_RUN_MODE: preprocess_data

  download_model:
    group: run-mode
    env:
      MLPERF_NVIDIA_RUN_COMMAND: download_model
      CM_MLPERF_NVIDIA_HARNESS_RUN_MODE: download_model
    deps:
      - tags: get,generic-python-lib,_torch_cuda
        enable_if_env:
          CM_MODEL:
            - retinanet

  build_engine:
    group: run-mode
    default_variations:
      loadgen-scenario: offline
    env:
      MLPERF_NVIDIA_RUN_COMMAND: generate_engines
      CM_MLPERF_NVIDIA_HARNESS_RUN_MODE: generate_engines
    deps:
      - tags: reproduce,mlperf,inference,nvidia,harness,_preprocess_data
        inherit_variation_tags: true
        force_cache: true
        skip_inherit_variation_groups:
          - run-mode
          - loadgen-scenario
          - device-memory
          - batch-size
      - tags: reproduce,mlperf,inference,nvidia,harness,_download_model
        inherit_variation_tags: true
        force_cache: true
        skip_inherit_variation_groups:
          - run-mode
          - loadgen-scenario
          - device-memory
          - batch-size
        skip_if_env:
          CM_MODEL:
            - retinanet_old
            - resnet50
            - bert-99
            - bert-99.9
            - dlrm-99
            - dlrm-99.9

  singlestream:
    group: loadgen-scenario
    env:
      CM_MLPERF_LOADGEN_SCENARIO: SingleStream
  multistream:
    group: loadgen-scenario
    env:
      CM_MLPERF_LOADGEN_SCENARIO: MultiStream
  offline:
    group: loadgen-scenario
    env:
      CM_MLPERF_LOADGEN_SCENARIO: Offline
  server:
    group: loadgen-scenario
    env:
      CM_MLPERF_LOADGEN_SCENARIO: Server

  run_harness:
    group: run-mode
    default: true
    default_variations:
      loadgen-scenario: offline
    deps:
      - tags: reproduce,mlperf,inference,nvidia,harness,_build_engine
        inherit_variation_tags: true
        names:
          - build-engine
        skip_inherit_variation_groups:
          - run-mode
          - gpu_name
          - device-memory
        force_cache: true
      - tags: reproduce,mlperf,inference,nvidia,harness,_preprocess_data
        inherit_variation_tags: true
        skip_inherit_variation_groups:
          - run-mode
          - loadgen-scenario
          - device-memory
          - gpu_name
          - batch-size
        force_cache: true
      - tags: reproduce,mlperf,inference,nvidia,harness,_download_model
        inherit_variation_tags: true
        skip_inherit_variation_groups:
          - run-mode
          - loadgen-scenario
          - device-memory
          - gpu_name
          - batch-size
        force_cache: true
        skip_if_env:
          CM_MODEL:
            - retinanet
            - resnet50
            - bert-99
            - bert-99.9
            - dlrm-99
            - dlrm-99.9
    env:
      CM_MLPERF_NVIDIA_HARNESS_RUN_MODE: run_harness
      MLPERF_NVIDIA_RUN_COMMAND: run_harness
    new_env_keys:
      - CM_MLPERF_*
      - CM_DATASET_*
      - CM_ML_MODEL_*
      - CM_HW_NAME
      - CM_MAX_EXAMPLES

  gpu_memory.16:
    group: device-memory
    env:
      CM_NVIDIA_GPU_MEMORY: "16"
  gpu_memory.24:
    group: device-memory
    env:
      CM_NVIDIA_GPU_MEMORY: "24"
  gpu_memory.8:
    group: device-memory
    env:
      CM_NVIDIA_GPU_MEMORY: "8"
  gpu_memory.32:
    group: device-memory
    env:
      CM_NVIDIA_GPU_MEMORY: "32"
  gpu_memory.40:
    group: device-memory
    env:
      CM_NVIDIA_GPU_MEMORY: "40"
  gpu_memory.48:
    group: device-memory
    env:
      CM_NVIDIA_GPU_MEMORY: "48"
  gpu_memory.80:
    group: device-memory
    env:
      CM_NVIDIA_GPU_MEMORY: "80"

  singlestream,run_harness:
    add_deps_recursive:
      build-engine:
        tags: _batch_size.1
    env:
      CM_MODEL_BATCH_SIZE: "1"
      CM_MLPERF_NVIDIA_HARNESS_GPU_BATCH_SIZE: "<<<CM_MODEL_BATCH_SIZE>>>"

  gpu_memory.16,bert_,offline,run_harness:
    add_deps_recursive:
      build-engine:
        tags: _batch_size.256
    env:
      CM_MODEL_BATCH_SIZE: "256"
      CM_MLPERF_NVIDIA_HARNESS_GPU_BATCH_SIZE: "<<<CM_MODEL_BATCH_SIZE>>>"

  gpu_memory.24,bert_,offline,run_harness:
    add_deps_recursive:
      build-engine:
        tags: _batch_size.256
    env:
      CM_MODEL_BATCH_SIZE: "256"
      CM_MLPERF_NVIDIA_HARNESS_GPU_BATCH_SIZE: "<<<CM_MODEL_BATCH_SIZE>>>"

  gpu_memory.80,bert_,offline,run_harness:
    add_deps_recursive:
      build-engine:
        tags: _batch_size.1024
    env:
      CM_MODEL_BATCH_SIZE: "1024"
      CM_MLPERF_NVIDIA_HARNESS_GPU_BATCH_SIZE: "<<<CM_MODEL_BATCH_SIZE>>>"

  gpu_memory.80,bert_,server,run_harness:
    add_deps_recursive:
      build-engine:
        tags: _batch_size.64
    env:
      CM_MODEL_BATCH_SIZE: "64"
      CM_MLPERF_NVIDIA_HARNESS_GPU_BATCH_SIZE: "<<<CM_MODEL_BATCH_SIZE>>>"

  gpu_memory.16,resnet50,offline,run_harness:
    add_deps_recursive:
      build-engine:
        tags: _batch_size.1024
    env:
      CM_MODEL_BATCH_SIZE: "1024"
      CM_MLPERF_NVIDIA_HARNESS_GPU_BATCH_SIZE: "<<<CM_MODEL_BATCH_SIZE>>>"
      CM_MLPERF_NVIDIA_HARNESS_GPU_COPY_STREAMS: "4"

  gpu_memory.40,resnet50,offline,run_harness:
    add_deps_recursive:
      build-engine:
        tags: _batch_size.2048
    env:
      CM_MODEL_BATCH_SIZE: "2048"
      CM_MLPERF_NVIDIA_HARNESS_GPU_BATCH_SIZE: "<<<CM_MODEL_BATCH_SIZE>>>"

  gpu_memory.24,resnet50,offline,run_harness:
    add_deps_recursive:
      build-engine:
        tags: _batch_size.64
    env:
      CM_MODEL_BATCH_SIZE: "64"
      CM_MLPERF_NVIDIA_HARNESS_GPU_BATCH_SIZE: "<<<CM_MODEL_BATCH_SIZE>>>"

  gpu_memory.80,resnet50,offline,run_harness:
    add_deps_recursive:
      build-engine:
        tags: _batch_size.2048
    env:
      CM_MODEL_BATCH_SIZE: "2048"
      CM_MLPERF_NVIDIA_HARNESS_GPU_BATCH_SIZE: "<<<CM_MODEL_BATCH_SIZE>>>"

  resnet50,multistream,run_harness:
    add_deps_recursive:
      build-engine:
        tags: _batch_size.8
    env:
      CM_MODEL_BATCH_SIZE: "8"
      CM_MLPERF_NVIDIA_HARNESS_GPU_BATCH_SIZE: "<<<CM_MODEL_BATCH_SIZE>>>"

  retinanet,multistream,run_harness:
    add_deps_recursive:
      build-engine:
        tags: _batch_size.1
    env:
      CM_MODEL_BATCH_SIZE: "1"
      CM_MLPERF_NVIDIA_HARNESS_GPU_BATCH_SIZE: "<<<CM_MODEL_BATCH_SIZE>>>"

  rnnt,multistream,run_harness:
    add_deps_recursive:
      build-engine:
        tags: _batch_size.8
    env:
      CM_MODEL_BATCH_SIZE: "8"
      CM_MLPERF_NVIDIA_HARNESS_GPU_BATCH_SIZE: "<<<CM_MODEL_BATCH_SIZE>>>"

  3d-unet,multistream,run_harness:
    add_deps_recursive:
      build-engine:
        tags: _batch_size.8
    env:
      CM_MODEL_BATCH_SIZE: "8"
      CM_MLPERF_NVIDIA_HARNESS_GPU_BATCH_SIZE: "<<<CM_MODEL_BATCH_SIZE>>>"

  dlrm,multistream,run_harness:
    add_deps_recursive:
      build-engine:
        tags: _batch_size.8
    env:
      CM_MODEL_BATCH_SIZE: "8"
      CM_MLPERF_NVIDIA_HARNESS_GPU_BATCH_SIZE: "<<<CM_MODEL_BATCH_SIZE>>>"

  gpu_memory.16,retinanet,offline,run_harness:
    add_deps_recursive:
      build-engine:
        tags: _batch_size.8
    env:
      CM_MODEL_BATCH_SIZE: "8"
      CM_MLPERF_NVIDIA_HARNESS_GPU_BATCH_SIZE: "<<<CM_MODEL_BATCH_SIZE>>>"

  gpu_memory.40,retinanet,offline,run_harness:
    add_deps_recursive:
      build-engine:
        tags: _batch_size.16
    env:
      CM_MODEL_BATCH_SIZE: "16"
      CM_MLPERF_NVIDIA_HARNESS_GPU_BATCH_SIZE: "<<<CM_MODEL_BATCH_SIZE>>>"

  gpu_memory.24,retinanet,offline,run_harness:
    add_deps_recursive:
      build-engine:
        tags: _batch_size.16
    env:
      CM_MODEL_BATCH_SIZE: "16"
      CM_MLPERF_NVIDIA_HARNESS_GPU_BATCH_SIZE: "<<<CM_MODEL_BATCH_SIZE>>>"

  gpu_memory.80,retinanet,offline,run_harness:
    add_deps_recursive:
      build-engine:
        tags: _batch_size.32
    env:
      CM_MODEL_BATCH_SIZE: "32"
      CM_MLPERF_NVIDIA_HARNESS_GPU_BATCH_SIZE: "<<<CM_MODEL_BATCH_SIZE>>>"

  gpu_memory.16,rnnt,offline,run_harness:
    add_deps_recursive:
      build-engine:
        tags: _batch_size.1024
    env:
      CM_MODEL_BATCH_SIZE: "1024"
      CM_MLPERF_NVIDIA_HARNESS_GPU_BATCH_SIZE: "<<<CM_MODEL_BATCH_SIZE>>>"

  gpu_memory.40,rnnt,offline,run_harness:
    add_deps_recursive:
      build-engine:
        tags: _batch_size.2048
    env:
      CM_MODEL_BATCH_SIZE: "2048"
      CM_MLPERF_NVIDIA_HARNESS_GPU_BATCH_SIZE: "<<<CM_MODEL_BATCH_SIZE>>>"

  gpu_memory.24,rnnt,offline,run_harness:
    add_deps_recursive:
      build-engine:
        tags: _batch_size.2048
    env:
      CM_MODEL_BATCH_SIZE: "2048"
      CM_MLPERF_NVIDIA_HARNESS_GPU_BATCH_SIZE: "<<<CM_MODEL_BATCH_SIZE>>>"

  gpu_memory.80,rnnt,offline,run_harness:
    add_deps_recursive:
      build-engine:
        tags: _batch_size.2048
    env:
      CM_MODEL_BATCH_SIZE: "2048"
      CM_MLPERF_NVIDIA_HARNESS_GPU_BATCH_SIZE: "<<<CM_MODEL_BATCH_SIZE>>>"

  gpu_memory.16,3d-unet_,offline,run_harness:
    add_deps_recursive:
      build-engine:
        tags: _batch_size.4
    env:
      CM_MODEL_BATCH_SIZE: "4"
      CM_MLPERF_NVIDIA_HARNESS_GPU_BATCH_SIZE: "<<<CM_MODEL_BATCH_SIZE>>>"

  gpu_memory.40,3d-unet_,offline,run_harness:
    add_deps_recursive:
      build-engine:
        tags: _batch_size.8
    env:
      CM_MODEL_BATCH_SIZE: "8"
      CM_MLPERF_NVIDIA_HARNESS_GPU_BATCH_SIZE: "<<<CM_MODEL_BATCH_SIZE>>>"

  gpu_memory.24,3d-unet_,offline,run_harness:
    add_deps_recursive:
      build-engine:
        tags: _batch_size.8
    env:
      CM_MODEL_BATCH_SIZE: "8"
      CM_MLPERF_NVIDIA_HARNESS_GPU_BATCH_SIZE: "<<<CM_MODEL_BATCH_SIZE>>>"

  gpu_memory.80,3d-unet_,offline,run_harness:
    add_deps_recursive:
      build-engine:
        tags: _batch_size.8
    env:
      CM_MODEL_BATCH_SIZE: "8"
      CM_MLPERF_NVIDIA_HARNESS_GPU_BATCH_SIZE: "<<<CM_MODEL_BATCH_SIZE>>>"

  gpu_memory.16,dlrm_,offline,run_harness:
    add_deps_recursive:
      build-engine:
        tags: _batch_size.315000
    env:
      CM_MODEL_BATCH_SIZE: "315000"
      CM_MLPERF_NVIDIA_HARNESS_GPU_BATCH_SIZE: "<<<CM_MODEL_BATCH_SIZE>>>"

  gpu_memory.40,dlrm_,offline,run_harness:
    add_deps_recursive:
      build-engine:
        tags: _batch_size.315000
    env:
      CM_MODEL_BATCH_SIZE: "315000"
      CM_MLPERF_NVIDIA_HARNESS_GPU_BATCH_SIZE: "<<<CM_MODEL_BATCH_SIZE>>>"

  gpu_memory.24,dlrm_,offline,run_harness:
    add_deps_recursive:
      build-engine:
        tags: _batch_size.315000
    env:
      CM_MODEL_BATCH_SIZE: "315000"
      CM_MLPERF_NVIDIA_HARNESS_GPU_BATCH_SIZE: "<<<CM_MODEL_BATCH_SIZE>>>"

  gpu_memory.80,dlrm_,offline,run_harness:
    add_deps_recursive:
      build-engine:
        tags: _batch_size.315000
    env:
      CM_MODEL_BATCH_SIZE: "315000"
      CM_MLPERF_NVIDIA_HARNESS_GPU_BATCH_SIZE: "<<<CM_MODEL_BATCH_SIZE>>>"

  rtx_4090:
    group: gpu-name
    env:
      CM_NVIDIA_CUSTOM_GPU: "yes"

  rtx_4090,resnet50,offline,run_harness:
    add_deps_recursive:
      build-engine:
        tags: _batch_size.64
    env:
      CM_MODEL_BATCH_SIZE: "64"
      CM_MLPERF_NVIDIA_HARNESS_GPU_BATCH_SIZE: "<<<CM_MODEL_BATCH_SIZE>>>"

  rtx_4090,retinanet,offline,run_harness:
    add_deps_recursive:
      build-engine:
        tags: _batch_size.4
    env:
      CM_MODEL_BATCH_SIZE: "4"
      CM_MLPERF_NVIDIA_HARNESS_GPU_BATCH_SIZE: "<<<CM_MODEL_BATCH_SIZE>>>"

  rtx_4090,bert_,offline,run_harness:
    add_deps_recursive:
      build-engine:
        tags: _batch_size.256
    env:
      CM_MODEL_BATCH_SIZE: "256"
      CM_MLPERF_NVIDIA_HARNESS_GPU_BATCH_SIZE: "<<<CM_MODEL_BATCH_SIZE>>>"

  rtx_4090,3d-unet_,offline,run_harness:
    add_deps_recursive:
      build-engine:
        tags: _batch_size.8
    env:
      CM_MODEL_BATCH_SIZE: "8"
      CM_MLPERF_NVIDIA_HARNESS_GPU_BATCH_SIZE: "<<<CM_MODEL_BATCH_SIZE>>>"

  rtx_4090,rnnt,offline,run_harness:
    add_deps_recursive:
      build-engine:
        tags: _batch_size.2048
    env:
      CM_MODEL_BATCH_SIZE: "2048"
      CM_MLPERF_NVIDIA_HARNESS_GPU_BATCH_SIZE: "<<<CM_MODEL_BATCH_SIZE>>>"

  rtx_4090,dlrm_,offline,run_harness:
    add_deps_recursive:
      build-engine:
        tags: _batch_size.315000
    env:
      CM_MODEL_BATCH_SIZE: "315000"
      CM_MLPERF_NVIDIA_HARNESS_GPU_BATCH_SIZE: "<<<CM_MODEL_BATCH_SIZE>>>"

  t4:
    group: gpu-name
    env:
      CM_NVIDIA_CUSTOM_GPU: "yes"

  t4,resnet50,offline,run_harness:
    add_deps_recursive:
      build-engine:
        tags: _batch_size.256
    env:
      CM_MODEL_BATCH_SIZE: "256"
      CM_MLPERF_NVIDIA_HARNESS_GPU_BATCH_SIZE: "<<<CM_MODEL_BATCH_SIZE>>>"

  t4,retinanet,offline,run_harness:
    add_deps_recursive:
      build-engine:
        tags: _batch_size.4
    env:
      CM_MODEL_BATCH_SIZE: "4"
      CM_MLPERF_NVIDIA_HARNESS_GPU_BATCH_SIZE: "<<<CM_MODEL_BATCH_SIZE>>>"

  t4,bert_,offline,run_harness:
    add_deps_recursive:
      build-engine:
        tags: _batch_size.256
    env:
      CM_MODEL_BATCH_SIZE: "256"
      CM_MLPERF_NVIDIA_HARNESS_GPU_BATCH_SIZE: "<<<CM_MODEL_BATCH_SIZE>>>"

  t4,3d-unet_,offline,run_harness:
    add_deps_recursive:
      build-engine:
        tags: _batch_size.8
    env:
      CM_MODEL_BATCH_SIZE: "8"
      CM_MLPERF_NVIDIA_HARNESS_GPU_BATCH_SIZE: "<<<CM_MODEL_BATCH_SIZE>>>"

  t4,rnnt,offline,run_harness:
    add_deps_recursive:
      build-engine:
        tags: _batch_size.2048
    env:
      CM_MODEL_BATCH_SIZE: "2048"
      CM_MLPERF_NVIDIA_HARNESS_GPU_BATCH_SIZE: "<<<CM_MODEL_BATCH_SIZE>>>"

  t4,dlrm_,offline,run_harness:
    add_deps_recursive:
      build-engine:
        tags: _batch_size.315000
    env:
      CM_MODEL_BATCH_SIZE: "315000"
      CM_MLPERF_NVIDIA_HARNESS_GPU_BATCH_SIZE: "<<<CM_MODEL_BATCH_SIZE>>>"

  pcie:
    group: gpu-connection

  sxm:
    group: gpu-connection

  a100:
    default_variation:
      gpu-connection: sxm
    group: gpu-name
    env:
      CM_NVIDIA_CUSTOM_GPU: "yes"

  a100,sxm,resnet50,offline,run_harness:
    add_deps_recursive:
      build-engine:
        tags: _batch_size.64
    env:
      CM_MODEL_BATCH_SIZE: "64"
      CM_MLPERF_NVIDIA_HARNESS_GPU_BATCH_SIZE: "<<<CM_MODEL_BATCH_SIZE>>>"

  a100,sxm,retinanet,offline,run_harness:
    add_deps_recursive:
      build-engine:
        tags: _batch_size.8
    env:
      CM_MODEL_BATCH_SIZE: "8"
      CM_MLPERF_NVIDIA_HARNESS_GPU_BATCH_SIZE: "<<<CM_MODEL_BATCH_SIZE>>>"

  a100,sxm,bert_,offline,run_harness:
    add_deps_recursive:
      build-engine:
        tags: _batch_size.256
    env:
      CM_MODEL_BATCH_SIZE: "256"
      CM_MLPERF_NVIDIA_HARNESS_GPU_BATCH_SIZE: "<<<CM_MODEL_BATCH_SIZE>>>"

  a100,sxm,3d-unet_,offline,run_harness:
    add_deps_recursive:
      build-engine:
        tags: _batch_size.8
    env:
      CM_MODEL_BATCH_SIZE: "8"
      CM_MLPERF_NVIDIA_HARNESS_GPU_BATCH_SIZE: "<<<CM_MODEL_BATCH_SIZE>>>"

  a100,sxm,rnnt,offline,run_harness:
    add_deps_recursive:
      build-engine:
        tags: _batch_size.2048
    env:
      CM_MODEL_BATCH_SIZE: "2048"
      CM_MLPERF_NVIDIA_HARNESS_GPU_BATCH_SIZE: "<<<CM_MODEL_BATCH_SIZE>>>"

  a100,sxm,dlrm_,offline,run_harness:
    add_deps_recursive:
      build-engine:
        tags: _batch_size.315000
    env:
      CM_MODEL_BATCH_SIZE: "315000"
      CM_MLPERF_NVIDIA_HARNESS_GPU_BATCH_SIZE: "<<<CM_MODEL_BATCH_SIZE>>>"

