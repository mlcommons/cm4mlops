# Identification of this CM script
alias: reproduce-mlperf-inference-nvidia
uid: bc3b17fb430f4732

automation_alias: script
automation_uid: 5b4e0237da074764

category: "Modular MLPerf benchmarks"


# User-friendly tags to find this CM script
tags:
  - reproduce
  - mlcommons
  - mlperf
  - inference
  - harness
  - nvidia-harness
  - nvidia

# Default environment
default_env:
  CM_BATCH_COUNT: '1'
  CM_BATCH_SIZE: '1'
  CM_FAST_COMPILATION: yes
  CM_MLPERF_LOADGEN_SCENARIO: Offline
  CM_MLPERF_LOADGEN_MODE: performance
  SKIP_POLICIES: '1'
  CM_NO_PREPROCESS_DATASET: no
  CM_SKIP_MODEL_DOWNLOAD: no
  CM_MLPERF_NVIDIA_RUN_COMMAND: run

# Map script inputs to environment variables
input_mapping:
  count: CM_MLPERF_LOADGEN_QUERY_COUNT
  max_batchsize: CM_MLPERF_LOADGEN_MAX_BATCHSIZE
  mlperf_conf: CM_MLPERF_CONF
  mode: CM_MLPERF_LOADGEN_MODE
  output_dir: CM_MLPERF_OUTPUT_DIR
  performance_sample_count: CM_MLPERF_LOADGEN_PERFORMANCE_SAMPLE_COUNT
  scenario: CM_MLPERF_LOADGEN_SCENARIO
  user_conf: CM_MLPERF_USER_CONF
  skip_preprocess: CM_SKIP_PREPROCESS_DATASET
  target_qps: CM_MLPERF_LOADGEN_TARGET_QPS
  offline_target_qps: CM_MLPERF_LOADGEN_OFFLINE_TARGET_QPS
  server_target_qps: CM_MLPERF_LOADGEN_SERVER_TARGET_QPS
  target_latency: CM_MLPERF_LOADGEN_TARGET_LATENCY
  singlestream_target_latency: CM_MLPERF_LOADGEN_SINGLESTREAM_TARGET_LATENCY
  multistream_target_latency: CM_MLPERF_LOADGEN_MULTISTREAM_TARGET_LATENCY
  use_triton: CM_MLPERF_NVIDIA_HARNESS_USE_TRITON
  gpu_copy_streams: CM_MLPERF_NVIDIA_HARNESS_GPU_COPY_STREAMS
  gpu_inference_streams: CM_MLPERF_NVIDIA_HARNESS_GPU_INFERENCE_STREAMS
  gpu_batch_size: CM_MLPERF_NVIDIA_HARNESS_GPU_BATCH_SIZE
  dla_copy_streams: CM_MLPERF_NVIDIA_HARNESS_DLA_COPY_STREAMS
  dla_inference_streams: CM_MLPERF_NVIDIA_HARNESS_DLA_INFERENCE_STREAMS
  dla_batch_size: CM_MLPERF_NVIDIA_HARNESS_DLA_BATCH_SIZE
  input_format: CM_MLPERF_NVIDIA_HARNESS_INPUT_FORMAT
  workspace_size: CM_MLPERF_NVIDIA_HARNESS_WORKSPACE_SIZE
  log_dir: CM_MLPERF_NVIDIA_HARNESS_LOG_DIR
  use_graphs: CM_MLPERF_NVIDIA_HARNESS_USE_GRAPHS
  run_infer_on_copy_streams: CM_MLPERF_NVIDIA_HARNESS_RUN_INFER_ON_COPY_STREAMS
  start_from_device: CM_MLPERF_NVIDIA_HARNESS_START_FROM_DEVICE
  end_on_device: CM_MLPERF_NVIDIA_HARNESS_END_ON_DEVICE
  max_dlas: CM_MLPERF_MAX_DLAS
  make_cmd: CM_MLPERF_NVIDIA_RUN_COMMAND

new_env_keys:
  - CM_MLPERF_*
  - CM_DATASET_*

# Dependencies on other CM scripts

deps:

  # Detect host OS features
  - tags: detect,os

  # Detect host CPU features
  - tags: detect,cpu

  # Install system dependencies on a given host
  - tags: get,sys-utils-cm

  # Detect CUDA
  - tags: get,cuda,_cudnn

  # Detect Tensorrt
  - tags: get,tensorrt

  # Build nvidia inference server
  - tags: build,nvidia,inference,server



  ########################################################################
  # Install MLPerf inference dependencies
  
  # Download MLPerf inference source
  - tags: get,mlcommons,inference,src
    names:
    - inference-src

  # Download Nvidia Submission Code
  - tags: get,nvidia,mlperf,inference,common-code,_custom
    names:
    - nvidia-inference-common-code


  ########################################################################
  # Install ResNet50 model (ONNX) and ImageNet

  - enable_if_env:
      CM_MODEL:
      - resnet50
    names:
      - imagenet-original
    tags: get,dataset,original,imagenet,_full


  ########################################################################
  # Install RetinaNet model (ONNX) and OpenImages

  - enable_if_env:
      CM_MODEL:
      - retinanet
    names:
      - openimages-original
    tags: get,dataset,original,openimages,_validation,_full

  - enable_if_env:
      CM_MODEL:
      - retinanet
    names:
      - openimages-calibration
    tags: get,dataset,original,openimages,_calibration


  - enable_if_env:
      CM_MODEL:
      - retinanet
    names:
      - ml-model-retinanet
    tags: get,ml-model,nvidia-retinanet,_efficient-nms

# Variations to customize dependencies
variations:
  # Target devices
  cpu:
    group: device
    default: true
    env:
      CM_MLPERF_DEVICE: cpu
  cuda:
    group: device
    env:
      CM_MLPERF_DEVICE: gpu
      CM_MLPERF_DEVICE_LIB_NAMESPEC: cudart

  tensorrt:
    group: backend
    default: true
    env:
      CM_MLPERF_BACKEND: tensorrt

  # Reference MLPerf models
  resnet50:
    group: model
    default: true
    env:
      CM_MODEL: resnet50

  retinanet:
    group: model
    env:
      CM_MODEL: retinanet
      CM_SKIP_MODEL_DOWNLOAD: yes

  bert-99:
    group: model
    env:
      CM_MODEL: bert-99

  bert-99.9:
    group: model
    env:
      CM_MODEL: bert-99.9

  batch_size.#:
    env:
      CM_MODEL_BATCH_SIZE: #
      CM_MLPERF_NVIDIA_HARNESS_GPU_BATCH_SIZE: #

  dla_batch_size.#:
    env:
      CM_MLPERF_NVIDIA_HARNESS_DLA_BATCH_SIZE: #
