# Identification of this CM script
alias: app-mlperf-inference
uid: d775cac873ee4231

automation_alias: script
automation_uid: 5b4e0237da074764

category: "Modular MLPerf benchmarks"
category_sort: 20000

developers: "[Arjun Suresh](https://www.linkedin.com/in/arjunsuresh), [Thomas Zhu](https://www.linkedin.com/in/hanwen-zhu-483614189), [Grigori Fursin](https://cKnowledge.io/@gfursin)"

# User-friendly tags to find this CM script
tags:
  - app
  - vision
  - language
  - mlcommons
  - mlperf
  - inference
  - generic

# Default environment
default_env:
  CM_BATCH_COUNT: '1'
  CM_BATCH_SIZE: '1'
  CM_MLPERF_LOADGEN_MODE: accuracy
  CM_MLPERF_LOADGEN_SCENARIO: Offline
  CM_OUTPUT_FOLDER_NAME: test_results
  CM_MLPERF_RUN_STYLE: test
  CM_TEST_QUERY_COUNT: '10'
  CM_MLPERF_QUANTIZATION: off

# Map script inputs to environment variables
input_mapping:
  count: CM_MLPERF_LOADGEN_QUERY_COUNT
  docker: CM_RUN_DOCKER_CONTAINER
  hw_name: CM_HW_NAME
  imagenet_path: IMAGENET_PATH
  max_batchsize: CM_MLPERF_LOADGEN_MAX_BATCHSIZE
  mode: CM_MLPERF_LOADGEN_MODE
  num_threads: CM_NUM_THREADS
  output_dir: OUTPUT_BASE_DIR
  power: CM_SYSTEM_POWER
  power_server: CM_MLPERF_POWER_SERVER_ADDRESS
  ntp_server: CM_MLPERF_POWER_NTP_SERVER
  max_amps: CM_MLPERF_POWER_MAX_AMPS
  max_volts: CM_MLPERF_POWER_MAX_VOLTS
  regenerate_files: CM_REGENERATE_MEASURE_FILES
  rerun: CM_RERUN
  scenario: CM_MLPERF_LOADGEN_SCENARIO
  test_query_count: CM_TEST_QUERY_COUNT
  new_tvm_model: CM_MLPERF_DELETE_COMPILED_MODEL
  clean: CM_MLPERF_CLEAN_SUBMISSION_DIR
  target_qps: CM_MLPERF_LOADGEN_TARGET_QPS
  target_latency: CM_MLPERF_LOADGEN_TARGET_LATENCY

# Duplicate CM environment variables to the ones used in native apps
env_key_mappings:
  CM_HOST_: HOST_
  CM_ML_: ML_
  CM_MLPERF_TVM: MLPERF_TVM

# Env keys which are exposed to higher level scripts
new_env_keys:
  - CM_MLPERF_*


# Dependencies on other CM scripts
deps:

  # Detect host OS features
  - tags: detect,os

  # Detect host CPU features
  - tags: detect,cpu

  # Install system dependencies on a given host
  - tags: get,sys-utils-cm

  # Detect/install python
  - tags: get,python
    names:
    - python
    - python3

  # Detect CUDA if required
  - tags: get,cuda
    enable_if_env:
      CM_MLPERF_DEVICE:
      - gpu




  ########################################################################
  # Install MLPerf inference dependencies

  # Download MLPerf inference source
  - tags: get,mlcommons,inference,src
    names:
    - inference-src

  # Get SUT configs (System Under Test)
  - tags: get,sut,configs



# Variations to customize dependencies
variations:
  # Implementation (cpp, python, nvidia, tflite-cpp)
  cpp:
    add_deps_recursive:
      imagenet-accuracy-script:
        tags: _int64
    env:
      CM_MLPERF_CPP: 'yes'
      CM_MLPERF_IMPLEMENTATION: cpp
    prehook_deps:
      - names:
         - cpp-mlperf-inference
         - mlperf-inference-implementation
        tags: app,mlperf,cpp,inference
        skip_if_env:
          CM_SKIP_RUN:
            - yes


  tflite-cpp:
    add_deps_recursive:
      imagenet-accuracy-script:
        tags: _int64
    env:
      CM_MLPERF_TFLITE_CPP: 'yes'
      CM_MLPERF_CPP: 'yes'
      CM_MLPERF_IMPLEMENTATION: tflite-cpp
    prehook_deps:
      - names:
         - tflite-cpp-mlperf-inference
         - mlperf-inference-implementation
        tags: app,mlperf,tflite-cpp,inference
        skip_if_env:
          CM_SKIP_RUN:
            - yes

  reference:
    add_deps_recursive:
      imagenet-accuracy-script:
        tags: _float32
    env:
      CM_MLPERF_PYTHON: 'yes'
      CM_MLPERF_IMPLEMENTATION: reference
    prehook_deps:
      - names:
         - python-reference-mlperf-inference
         - mlperf-inference-implementation
        tags: app,mlperf,reference,inference
        skip_if_env:
          CM_SKIP_RUN:
            - yes
  python:
    alias: reference
  nvidia:
    add_deps_recursive:
      imagenet-accuracy-script:
        tags: _float32
      python:
        version-min: 3.8.0
    env:
      CM_MLPERF_IMPLEMENTATION: nvidia
    deps:
      ## Nvidia common code
      - tags: get,mlperf,inference,nvidia,common-code
      - tags: get,mlperf,training,src
      - tags: get,generic-python-lib,_nvidia-pyindex
      - tags: get,generic-python-lib,_nvidia-tensorrt
      - tags: get,generic-python-lib,_numpy
      - tags: get,generic-python-lib,_pycuda
      - tags: get,generic-python-lib,_mlperf_logging
      - tags: get,generic-python-lib,_onnx
  
  resnet50:
    group:
      models
    default:
      true
    env:
      CM_MODEL:
        resnet50
    add_deps_recursive:
      mlperf-inference-implementation:
        tags: _resnet50
    deps: 
    - tags: get,dataset-aux,imagenet-aux
    post_deps:
    - enable_if_env:
        CM_MLPERF_LOADGEN_MODE:
        - accuracy
        - all
        CM_MLPERF_ACCURACY_RESULTS_DIR:
        - 'on'
      names:
      - mlperf-accuracy-script
      - imagenet-accuracy-script
      tags: run,accuracy,mlperf,_imagenet

  retinanet:
    group:
      models
    env:
      CM_MODEL:
        retinanet
    add_deps_recursive:
      mlperf-inference-implementation:
        tags: _retinanet
    post_deps:
    - enable_if_env:
        CM_MLPERF_LOADGEN_MODE:
        - accuracy
        - all
        CM_MLPERF_ACCURACY_RESULTS_DIR:
        - 'on'
      names:
      - mlperf-accuracy-script
      - openimages-accuracy-script
      tags: run,accuracy,mlperf,_openimages

  3d-unet-99:
    group:
      models
    base:
    - 3d-unet_
    env:
      CM_MODEL:
        3d-unet-99
    add_deps_recursive:
      mlperf-inference-implementation:
        tags: _3d-unet-99
  3d-unet-99.9:
    group:
      models
    base:
    - 3d-unet_
    env:
      CM_MODEL:
        3d-unet-99.9
    add_deps_recursive:
      mlperf-inference-implementation:
        tags: _3d-unet-99.9

  3d-unet_:
    post_deps:
    - enable_if_env:
        CM_MLPERF_LOADGEN_MODE:
        - accuracy
        - all
        CM_MLPERF_ACCURACY_RESULTS_DIR:
        - 'on'
      names:
      - mlperf-accuracy-script
      - 3dunet-accuracy-script
      tags: run,accuracy,mlperf,_3dunet

  rnnt:
    group:
      models
    env:
      CM_MODEL:
        rnnt
    add_deps_recursive:
      mlperf-inference-implementation:
        tags: _rnnt
    post_deps:
    - enable_if_env:
        CM_MLPERF_LOADGEN_MODE:
        - accuracy
        - all
        CM_MLPERF_ACCURACY_RESULTS_DIR:
        - 'on'
      names:
      - mlperf-accuracy-script
      - rnnt-accuracy-script
      tags: run,accuracy,mlperf,_rnnt

  bert_:
    post_deps:
    - enable_if_env:
        CM_MLPERF_LOADGEN_MODE:
        - accuracy
        - all
        CM_MLPERF_ACCURACY_RESULTS_DIR:
        - 'on'
      names:
      - squad-accuracy-script
      - mlperf-accuracy-script
      tags: run,accuracy,mlperf,_squad,_float32
    add_deps_recursive:
      inference-src:
        tags: _deeplearningexamples
  bert-99:
    group:
      models
    base:
    - bert_
    env:
      CM_MODEL:
        bert-99
    add_deps_recursive:
      mlperf-inference-implementation:
        tags: _bert-99
  bert-99.9:
    group:
      models
    base:
    - bert_
    env:
      CM_MODEL:
        bert-99.9
    add_deps_recursive:
      mlperf-inference-implementation:
        tags: _bert-99.9


  onnxruntime:
    group: backend
    default: true
    env:
      CM_MLPERF_BACKEND:
        onnxruntime
    add_deps_recursive:
      mlperf-inference-implementation:
        tags: _onnxruntime
  tf:
    group: backend
    env:
      CM_MLPERF_BACKEND:
        tf
    add_deps_recursive:
      mlperf-inference-implementation:
        tags: _tf
  pytorch:
    group: backend
    env:
      CM_MLPERF_BACKEND:
        pytorch
    add_deps_recursive:
      mlperf-inference-implementation:
        tags: _pytorch
  deepsparse:
    group: backend
    env:
      CM_MLPERF_BACKEND:
        deepsparse
    add_deps_recursive:
      mlperf-inference-implementation:
        tags: _deepsparse
  tflite:
    group: backend
    env:
      CM_MLPERF_BACKEND: tflite
    add_deps_recursive:
      mlperf-inference-implementation:
        tags: _tflite
  tvm-onnx:
    group: backend
    env:
      CM_MLPERF_BACKEND: tvm-onnx
    add_deps_recursive:
      mlperf-inference-implementation:
        tags: _tvm-onnx
  tvm-pytorch:
    group: backend
    env:
      CM_MLPERF_BACKEND: tvm-pytorch
    add_deps_recursive:
      mlperf-inference-implementation:
        tags: _tvm-pytorch

  cpu:
    group:
      device
    default:
      True
    env:
      CM_MLPERF_DEVICE:
        cpu
    add_deps_recursive:
      mlperf-inference-implementation:
        tags: _cpu
  cuda:
    group:
      device
    env:
      CM_MLPERF_DEVICE:
        gpu
    add_deps_recursive:
      mlperf-inference-implementation:
        tags: _cuda


  # Execution modes
  fast:
    group: execution-mode
    env:
      CM_FAST_FACTOR: '5'
      CM_OUTPUT_FOLDER_NAME: fast_results
      CM_MLPERF_RUN_STYLE: fast

  test:
    group: execution-mode
    default: true
    env:
      CM_OUTPUT_FOLDER_NAME: test_results
      CM_MLPERF_RUN_STYLE: test

  valid:
    group: execution-mode
    env:
      CM_OUTPUT_FOLDER_NAME: valid_results
      CM_MLPERF_RUN_STYLE: valid

  # Turn on quantization
  quantized:
    env:
      CM_MLPERF_QUANTIZATION: on

  power:
    env:
      CM_MLPERF_POWER: yes
      CM_SYSTEM_POWER: yes
    add_deps_recursive:
      runner:
        tags:
          _mlperf-power

  # Reproducibility (past submissions)
  r2.1_default:
    add_deps_recursive:
      compiler:
        tags: llvm
      inference-src:
        tags: _octoml
      loadgen:
        version: r2.1
    env:
      CM_RERUN: 'yes'
      CM_SKIP_SYS_UTILS: 'yes'
      CM_TEST_QUERY_COUNT: '100'

valid_variation_combinations:
  -
    - resnet50
    - cpu
  -
    - resnet50
    - cuda
  -
    - retinanet
  -
    - bert-99.9
  -
    - bert-99
  -
    - rnnt
  -
    - 3d-unet-99
  -
    - 3d-unet-99.9

invalid_variation_combinations:
  -
    - resnet50
    - pytorch
  -
    - retinanet
    - tf
