### Challenge

Reproduce and automate [TinyMLPerf benchmarks](https://github.com/mlcommons/tiny).

Join this public [Discord server](https://discord.gg/JjWNWXKxwT) 
to discuss with the community and organizers 
how to use and enhance CK to run and optimize MLPerf inference benchmarks.

### Organizers

* [MLCommons taskforce on automation and reproducibility](https://cKnowledge.org/mlcommons-taskforce)
* [cTuning foundation](https://cTuning.org)
* [cKnowledge Ltd](https://cKnowledge.org)

### Status

We have successfully reproduced [OctoML's v1.0 submission](https://github.com/mlcommons/tiny_results_v1.0/tree/main/closed/OctoML),
automated it with the latest version of the [MLCommons CM automation language](https://github.com/mlcommons/ck/blob/master/docs/README.md),
submit reproduce results to [TinyMLperf v1.1 round] 
and added all past TinyMLPerf results to the [MLCommons CK playground](https://access.cknowledge.org/playground/?action=experiments&tags=mlperf-tiny)
for further collaborative analysis and improvement.

Our tutorial and reproducibility report is available [here](https://github.com/ctuning/mlcommons-ck/blob/master/docs/tutorials/reproduce-mlperf-tiny.md).

TinyMLPerf v1.1 results will be public at the end of June 2023.

### Related discussions for the future

* https://github.com/mlcommons/ck/pull/693
* https://github.com/mlcommons/ck/pull/700
* https://github.com/mlcommons/ck/pull/701

### Results

All results will be available in [this GitHub repo](https://github.com/ctuning/cm_inference_results)
and can be visualized and compared using the [MLCommons Collective Knowledge Playground](https://access.cknowledge.org/playground/?action=experiments&tags=mlperf-tiny).
