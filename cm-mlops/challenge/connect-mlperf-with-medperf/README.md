### Challenge

Evaluate models from [MLCommons MedPerf platform](https://www.medperf.org) in terms of latency, throughput, power consumption and other metrics
using MLPerf loadgen and MLCommons CM automation language. 

See the [Nature 2023 article about MedPerf](https://www.nature.com/articles/s42256-023-00652-2)
and [ACM REP'23 keynote about CM](https://doi.org/10.5281/zenodo.8105339) to learn more about these projects.

Join our public [Discord server](https://discord.gg/JjWNWXKxwT) to discuss this challenge with the organizers.

Read [this documentation](https://github.com/mlcommons/ck/blob/master/docs/mlperf/inference/README.md) 
to run reference implementations of MLPerf inference benchmarks 
using the CM automation language and use them as a base for your developments.


### Prizes

* *All contributors will participate in writing a common white paper.*
* *All contributors will receive an official MLCommons Collective Knowledge contributor award (see [this example](https://ctuning.org/awards/ck-award-202307-zhu.pdf)).*


### Organizers

* [cKnowledge.org](https://www.linkedin.com/company/cknowledge)
* [cTuning.org](https://www.linkedin.com/company/ctuning-foundation)
* [MLCommons](https://cKnowledge.org/mlcommons-taskforce)
