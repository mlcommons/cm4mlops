<!------------------------------------------------>
<center>
 <span style="font-size:small">
  [ <a href="http://asplos-conference.org">Back to the ASPLOS 2020 conference website</a> ]
 </span>
</center>

<!------------------------------------------------>
<h4>Results</h4>

 <ul>
  <li>
   <a href="https://cknow.io/?q=%22reproduced-papers%22+AND+asplos2020">Papers which successfully passed artifact evaluation</a>
  </li>
 </ul>

<!------------------------------------------------>
<h4>Important dates</h4>

<div style="margin-left:20px;">
 Paper decision: <b><strike>November 20, 2019</strike></b><br>
 Artifact submission: <a href="https://asplos20ae.hotcrp.com"><b><strike>December 4, 2019</strike></b></a><br>
 Artifact decision: <b><strike>January 15, 2020</strike></b><br>
 Camera-ready paper: <b><strike>January 20, 2020</strike></b><br>
 Conference: <a href="http://asplos-conference.org"><b><strike>March 16-20, 2020</strike></b></a><br>
</div>

<!------------------------------------------------>
<h4>Reproducibility chairs</h4>

<div style="margin-left:20px;">
 <ul>
  <li><a href="https://fursin.net/research.html">Grigori Fursin</a> (cTuning foundation)<br>
  <li><a href="https://scholar.harvard.edu/vijay-janapa-reddi/home">Vijay Janapa Reddi</a> (Harvard University)<br>
  <li><a href="https://homes.cs.washington.edu/~moreau">Thierry Moreau</a> (University of Washington)<br>
 </ul>
</div>

<!------------------------------------------------>
<h4>Motivation</h4>

<div style="margin-left:20px;">

 Authors of accepted <a href="https://asplos-conference.org">ASPLOS'20</a> papers 
 are invited to formally submit their supporting materials (code, data, models, workflows, results) 
 to the <a href="https://cTuning.org/ae/submission.html">Artifact Evaluation process (AE)</a>.

 AE is run by a separate committee whose task is to <a href="http://ctuning.org/ae/reviewing.html">assess</a>
 how submitted artifacts support the work described in accepted papers while reproducing at least some experiments. 
 <b>This submission is voluntary and will not influence the final decision regarding the papers.</b>

 <br><br>
 Since it is not always trivial to perform a full validation of computer architecture experiments
 and may require expensive computational resources, we use multistage Artifact Evaluation:
 we will validate only the "availability" and "functionality/reusability" of submitted
 artifacts at ASPLOS'20.
 Thus, depending on evaluation results, camera-ready papers will include the Artifact Appendix 
 and will receive at most two ACM stamps of approval printed on the first page (note that authors 
 still need to provide a small sample dataset to test the functionality of their artifacts):

 <br><br>
  <center>
   <table border="1">
    <tr>
     <td>
      <a href="http://ctuning.org/ae/reviewing.html#artifacts_available"><img src="https://www.acm.org/binaries/content/gallery/acm/publications/replication-badges/artifacts_available_dl.jpg" width="70"></a>
     </td>
     <td>
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
     </td>
     <td>
      <a href="http://ctuning.org/ae/reviewing.html#artifacts_functional"><img src="https://www.acm.org/binaries/content/gallery/acm/publications/replication-badges/artifacts_evaluated_functional_dl.jpg" width="70"></a>
     </td>
     <td>
      or
     </td>
     <td>
      <a href="http://ctuning.org/ae/reviewing.html#artifacts_reusable"><img src="https://www.acm.org/binaries/content/gallery/acm/publications/replication-badges/artifacts_evaluated_reusable_dl.jpg" width="70"></a>
     </td>
    </tr> 
   </table>
  </center>

 <br>
 We think that the second AE stage can be a special reproducibility session or an open tournament
 to perform a full validation of experimental results from above papers 
 with artifacts at the next conference based on the successful 
 <a href="https://cKnowledge.org/request.html">ASPLOS-ReQuEST'19 experience</a>.
 It is under discussion so feel free to provide your feedback
 to the ASPLOS AE chairs!

</div> 


<!------------------------------------------------>
<br>
<a name="evaluators"></a>
<h4>Artifact Evaluation Committee</h4>

<div style="margin-left:20px;">
 <ul>
  <li>Maaz Bin Safeer Ahmad (University of Washington)</li>
  <li>Ismail Akturk (University of Missouri, Columbia)</li>
  <li>Basavesh Ammanaghatta Shivakumar (Purdue University)</li>
  <li>Vinay Banakar (Hewlett Packard Labs)</li>
  <li>Gilbert Bernstein (UC Berkeley)</li>
  <li>Abhishek Bhattacharyya (U. Wisconsin)</li>
  <li>James Bornholt (University of Texas at Austin)</li>
  <li>Rangeen Basu Roy Chowdhury (Intel Corporation)</li>
  <li>Tapan Chugh (University of Washington)</li>
  <li>Basile Clément (École Normale Supérieure and Inria)</li>
  <li>Alexei Colin (USC Information Sciences Institute)</li>
  <li>Weilong Cui (Google)</li>
  <li>Deeksha Dangwal (University of California, Santa Barbara)</li>
  <li>Davide Del Vento (UCAR)</li>
  <li>Murali Emani (Argonne National Laboratory)</li>
  <li>Vitor Enes (HASLab / INESC TEC and Universidade do Minho)</li>
  <li>Haggai Eran (Technion - Israel Institute of Technology & Mellanox Technologies)</li>
  <li>Ata Fatahi (Penn State University)</li>
  <li>Swapnil Gandhi (Indian Institute of Science (IISc))</li>
  <li>Hervé Guillou (CodeReef & cTuning foundation)</li>
  <li>Faruk Guvenilir (Microsoft, The University of Texas at Austin)</li>
  <li>Bastian Hagedorn (University of Münster, Germany)</li>
  <li>Kartik Hegde (UIUC)</li>
  <li>Qijing Huang (UC Berkeley)</li>
  <li>Sergio Iserte (Universitat Jaume I)</li>
  <li>Shehbaz Jaffer (University of Toronto)</li>
  <li>Tanvir Ahmed Khan (University of Michigan)</li>
  <li>Sung Kim (University of Michigan)</li>
  <li>Marios Kogias (EPFL)</li>
  <li>Iacovos G. Kolokasis (University of Crete and FORTH-ICS)</li>
  <li>Tzu-Mao Li (UC Berkeley)</li>
  <li>Chien-Yu Lin (U. Washington)</li>
  <li>Hongyuan Liu (College of William and Mary)</li>
  <li>Sihang Liu (University of Virginia)</li>
  <li>Joseph McMahan (University of Washington)</li>
  <li>Fatemeh Mireshghallah (University of California-San Diego)</li>
  <li>Thierry Moreau (University of Washington)</li>
  <li>Chandrakana Nandi (University of Washington, Seattle)</li>
  <li>Mohammad Nasirifar (U. Toronto)</li>
  <li>Asmita Pal (University of Wisconsin-Madison)</li>
  <li>Pratyush Patel (University of Washington)</li>
  <li>Arash Pourhabibi (EPFL)</li>
  <li>Thamir Qadah (Purdue University, West Lafayette)</li>
  <li>Alexander Reinking (University of California Berkeley)</li>
  <li>Emily Ruppel (Carnegie Mellon University)</li>
  <li>Gururaj Saileshwar (Georgia Institute of Technology)</li>
  <li>Solmaz Salimi (Sharif University of Technology (SUT))</li>
  <li>Gus Smith (U. Washington)</li>
  <li>Linghao Song (Duke University)</li>
  <li>Akshitha Sriraman (University of Michigan)</li>
  <li>Tom St. John (Tesla)</li>
  <li>Pengfei Su (College of William and Mary)</li>
  <li>Arun Subramaniyan (University of Michigan)</li>
  <li>Mark Sutherland (EPFL)</li>
  <li>Iman Tabrizian (University of Toronto)</li>
  <li>Dmitrii Ustiugov (EPFL)</li>
  <li>Jyothi Vedurada (IIT Madras)</li>
  <li>Di Wu (Department of ECE, University of Wisconsin-Madison)</li>
  <li>Xi Yang (University of Sydney)</li>
  <li>Felippe Zacarias (UPC/BSC)</li>
  <li>Rui Zhang (Ohio State University)</li>
  <li>Fang Zhou (Ohio State University)</li>
 </ul>
</div>

<!------------------------------------------------>
<h4>Public discussion</h4>

<div style="margin-left:20px;">
 We plan to organize an open session at ASPLOS to discuss artifact evaluation results and a common methodology to perform a full validation and comparison of computer architecture experiments 
 (see related SIGARCH blogs <a href="https://www.sigarch.org/a-checklist-manifesto-for-empirical-evaluation-a-preemptive-strike-against-a-replication-crisis-in-computer-science">"A Checklist Manifesto for Empirical Evaluation: A Preemptive Strike Against a Replication Crisis in Computer Science"</a> 
 and <a href="https://www.sigarch.org/artifact-evaluation-for-reproducible-quantitative-research">"Artifact Evaluation for Reproducible Quantitative Research"</a>).
</div>

<!------------------------------------------------>
<h4>Artifact submission</h4>

<div style="margin-left:20px;">
 Prepare your submission and this <a href="https://github.com/ctuning/ck-artifact-evaluation/blob/master/wfe/artifact-evaluation/templates/ae-20190108.tex">Artifact Appendix</a> 
 using the following <a href="http://ctuning.org/ae/submission.html">guidelines</a> and register it at the 
 <a href="https://asplos20ae.hotcrp.com">ASPLOS'20 AE website</a>. 
 Your submission will be then reviewed according to the <a href="http://ctuning.org/ae/reviewing.html">following guidelines</a>.
 <b>Please, do not forget to provide a list of hardware, software, benchmark and data set dependencies 
 in your artifact abstract - this is essential to find appropriate evaluators!</b>

 <br><br>
 The papers that successfully go through AE will receive a set of ACM badges of approval printed on the papers themselves 
 and available as meta information in the <a href="http://www.acm.org/publications/policies/artifact-review-badging">ACM Digital Library</a> 
 (it is now possible to search for papers with specific badges in ACM DL). 
 Authors of such papers will have an option to include up to 2 pages of their Artifact Appendix to the camera-ready paper. 

 <br><br>
 At the end of the process we will inform you about how to add badges 
 to your camera-ready paper.

</div>


<!------------------------------------------------>
<h4>Questions and feedback</h4>

<div style="margin-left:20px;">
Please check <a href="https://cTuning.org/ae/faq.html">AE FAQs</a>
and feel free to ask questions or provide your feedback and suggestions
via the <a href="https://groups.google.com/forum/#!forum/artifact-evaluation">dedicated AE discussion group</a>.
</div>

<br>
