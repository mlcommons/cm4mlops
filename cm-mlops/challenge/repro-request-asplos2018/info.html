<p>Results of the 1st reproducible ACM ReQuEST-ASPLOS'18 tournament:</p>

 <ul>
   <li><a href="https://cknow.io/?q=%22papers-request%22">Papers</a>
   <li><a href="https://portalparts.acm.org/3230000/3229762/fm/frontmatter.pdf">Organizers' report</a>
   <li><a href="https://cknow.io/c/result/pareto-efficient-ai-co-design-tournament-request-acm-asplos-2018/">REQUEST live dashboard</a>
   <li><a href="https://dl.acm.org/citation.cfm?doid=3229762">ACM proceedings with reproducibility badges</a>
   <li><a href="https://github.com/ctuning/ck">CK framework</a>
   <li><a href="https://github.com/ctuning/ck-request-asplos18-results">Portable, customizable and reusable CK workflows from this tournament</a>
   <li><a href="https://cknow.io/repos">Shared CK repositories</a>
   <li><a href="http://cknowledge.org/android-demo.html">Android app to crowdsource AI/ML/SW/HW benchmarking and co-design</a>
   <li><a href="https://groups.google.com/forum/#!forum/collective-knowledge">Discussion group</a>
 </ul>

<h3>Goals</h3>

Our long-term goal is to develop a common methodology and framework for
reproducible co-design of the efficient software/hardware stack for
emerging algorithms requested by our advisory board (inference, object
detection, training, etc) in terms of speed, accuracy, energy, size,
complexity, costs and other metrics.

Open REQUEST competitions bring together AI, ML and systems researchers
to share complete algorithm implementations (code and data) as <a
href="https://github.com/ctuning/ck/wiki/Portable-workflows">portable</a>,
customizable and reusable
<a href="https://cknow.io/programs">Collective Knowledge
workflows</a>.

This helps other researchers and end-users to quickly validate such
results, reuse workflows and optimize/autotune algorithms across different
platforms, models, data sets, libraries, compilers and tools.

We will also use our practical experience reproducing experimental results
from REQUEST submissions to help set up artifact evaluation at the
upcoming <a href="https://mlsys.cc">MLSys</a>, and to suggest new
algorithms for the inclusion to the <a href="https://mlperf.org">MLPerf
benchmark</a>.

<p>
The associated ACM ReQuEST workshop is co-located with <a href="https://www.asplos2018.org">ASPLOS 2018</a> March 24th, 2018 (afternoon), Williamsburg, VA, USA.

<p>
A ReQuEST introduction and long-term goals: <a href="https://cknow.io/c/event/request-reproducible-benchmarking-tournament">cKnowledge.org/request website</a> 
and <a href="https://arxiv.org/abs/1801.06378">ArXiv paper</a>.



<h3>Steering committee (A-Z)</h3>

  <ul>
      <li><a href="https://www.cs.washington.edu/people/faculty/luisceze">Luis Ceze</a>, University of Washington, USA
      <li><a href="http://www.eecg.toronto.edu/~enright">Natalie Enright Jerger</a>, University of Toronto, Canada
      <li><a href="https://parsa.epfl.ch/~falsafi">Babak Falsafi</a>, EPFL, Switzerland
      <li><a href="https://fursin.net/research.html">Grigori Fursin</a>, cTuning foundation, France and dividiti, UK (<b>co-organizer</b>)
      <li><a href="https://uk.linkedin.com/in/lokhmotov">Anton Lokhmotov</a>, dividiti, UK
      <li><a href="https://homes.cs.washington.edu/~moreau">Thierry Moreau</a>, University of Washington, USA (<b>co-organizer</b>)
      <li><a href="http://www.cs.cornell.edu/~asampson">Adrian Sampson</a>, Cornell University, USA
      <li><a href="https://www.energy.cam.ac.uk/directory/ps751@cam.ac.uk">Phillip Stanley Marbell</a>, University of Cambridge, UK
  </ul>

<h3>Advisory/industrial board (A-Z)</h3>

  <ul>
    <li><a href="https://ie.linkedin.com/in/michaelablott">Michaela Blott</a>, Xilinx
    <li><a href="https://www.linkedin.com/in/unmeshdbordoloi">Unmesh Bordoloi</a>, General Motors
    <li><a href="https://www.microsoft.com/en-us/research/people/oferd/">Ofer Dekel</a>, Microsoft
    <li><a href="http://openlab.cern/about/people/maria-girone">Maria Girone</a>, CERN openlab
    <li><a href="https://www.linkedin.com/in/waynegraves">Wayne Graves</a>, ACM
    <li><a href="https://www.linkedin.com/in/vinodg">Vinod Grover</a>, NVIDIA
    <li><a href="https://www.linkedin.com/in/sumitg">Sumit Gupta</a>, IBM
    <li><a href="http://research.nvidia.com/person/stephen-keckler">Steve Keckler</a>, NVIDIA
    <li><a href="https://www.linkedin.com/in/wei-li-a4a611">Wei Li</a>, Intel
    <li><a href="https://www.linkedin.com/in/colin-osborne-7129362/">Colin Osborne</a>, Arm
    <li><a href="https://www.microsoft.com/en-us/research/people/anputnam">Andrew Putnam</a>, Microsoft
    <li><a href="https://archive.pioneers.io/blog/people/boris-shulkin">Boris Shulkin</a>, Magna
    <li><a href="https://www.linkedin.com/in/greg-stoner-17830">Greg Stoner</a>, AMD
    <li><a href="https://www.linkedin.com/in/alexwade">Alex Wade</a>, Chan Zuckerberg Initiative
    <li><a href="https://www.linkedin.com/in/peng-wu-411b5b">Peng Wu</a>, Huawei
    <li><a href="https://research.google.com/pubs/105499.html">Cliff Young</a>, Google
  </ul>

<h3>Partners</h3>

<center>
     <a href="https://www.cam.ac.uk/">
         <img style="height:70px;" src="https://cKnowledge.org/_resources/partners/logo-university-of-cambridge.png" />
     </a>
    <a href="https://www.cornell.edu">
        <img style="height:70px;" src="https://cKnowledge.org/_resources/partners/logo-cornell-university.png" />
    </a>
    <a href="https://www.utoronto.ca/">
        <img style="height:70px;" src="https://cKnowledge.org/_resources/partners/logo-university-of-toronto.png" />
    </a>
    <a href="https://www.washington.edu">
        <img style="height:70px;" src="https://cKnowledge.org/_resources/partners/logo-university-of-washington.png" />
    </a>
    <a href="https://www.epfl.c">
        <img style="height:70px;" src="https://cKnowledge.org/_resources/partners/logo-epfl.png" />
    </a>
    <a href="http://sigarch.org">
      <img style="height:70px;" src="https://cKnowledge.org/_resources/partners/logo-sigarch.png" />
    </a>
    <a href="http://acm.org">
      <img style="height:70px;" src="https://cKnowledge.org/_resources/partners/logo-acm.png" />
    </a>
    <a href="http://dividiti.com">
        <img style="height:70px;" src="https://cKnowledge.org/_resources/partners/logo-dvdt.png" />
    </a>
    <a href="http://cTuning.org">
        <img style="height:70px;" src="https://cKnowledge.org/_resources/partners/logo-ctuning-foundation.png" />
    </a>
</center>

        <div class="container">
            <h2 class="pt-5">Workshop program</h2>
        </div>

      <div class="container container-wrapped">
          <div class="row d-none d-lg-flex">
              <div class="col-2 table-title">
                Time slot
              </div>
              <div class="col-10 col-lg-8 table-title">
                Presentation
              </div>
              <div class="d-none d-lg-block col-lg-2 table-title">
                Reusable artifacts
              </div>
          </div>

          <div class="row">
              <div class="d-none d-lg-block col-lg-2 table-cell">
                <h3>1:30pm—1:40pm</h3>
              </div>
              <div class="col-12 col-lg-8 table-cell">
                <h6 class="d-block d-lg-none">1:30pm—1:40pm</h6>
                <h3>Workshop introduction</h3>
                <p>ReQuEST tournaments bring together multidisciplinary researchers (AI, ML, systems) to find the most efficient solutions for realistic problems requested by the advisory board in terms of speed, accuracy, energy, complexity, costs and other metrics across the whole application/software/hardware stack In a fair and reproducible way. All the winning solutions (code, data, workflow) on a Pareto-frontier are then available to the community as portable and customizablelug&play" AI/ML components with a common API and meta information. The ultimate goal is to accelerate research and reduce costs by reusing the most accurate and efficient AI/ML blocks continuously optimized, autotuned and crowd-tuned across diverse models, data sets and platforms from a cloud to edge.</p>
              </div>
              <div class="d-none d-lg-block col-lg-2 table-cell">
              </div>
          </div>

          <div class="row">
              <div class="d-none d-lg-block col-lg-2 table-cell">
                <h3>1:40pm—2:30pm</h3>
              </div>
              <div class="col-12 col-lg-8 table-cell">
                <h6 class="d-block d-lg-none">1:40pm—2:30pm</h6>
                <h3>Keynote "The Retrospect and Prospect of Low-Power Image Recognition Challenge (LPIRC)"</h3>
                <p><a href="http://ei-lab.org/people/faculty/yiran-chen-3">Prof. Yiran Chen</a>, Duke University, USA</p>
                <p><a href="http://cKnowledge.org/request/request_asplos18_keynote_final_version.pdf">Slides in PDF</a></p>
                <img class="table-img" src="_resources/photo-yiran-chen-reduced-min.jpg" style="float:right;padding:0 0 8px 24px;">
                <p><b>Abstract:</b> Reducing power consumption has been one of the most important goals since the creation of electronic systems.
                      Energy efficiency is increasingly important as battery-powered systems (such as smartphones, drones, and body cameras) are widely used.
                      It is desirable using the on-board computers to recognize objects in the images captured by these cameras.
                      The Low-Power Image Recognition Challenge (LPIRC) is an annual competition started in 2015, aiming to discover the best technology in both image recognition and energy conservation.
                      In this talk, we will explains the rules of the competition and the rationale, summarizes the teams' scores, and describes the lessons learned in the past years.
                      We will also discuss possible improvements of future challenges and collaboration opportunities with other events and competitions like ReQuEST.
                </p>
                <p><b>Short bio:</b> Yiran Chen received B.S and M.S. from Tsinghua University and Ph.D. from Purdue University in 2005.
                      After five years in industry, he joined University of Pittsburgh in 2010 as Assistant Professor and then promoted to Associate Professor with tenure in 2014, held Bicentennial Alumni Faculty Fellow.
                      He now is a tenured Associate Professor of the Department of Electrical and Computer Engineering at Duke University and serving as the co-director of Duke Center for Evolutionary Intelligence (CEI), focusing on the research of new memory and storage systems, machine learning and neuromorphic computing, and mobile computing systems.
                      Dr. Chen has published one book and more than 300 technical publications and has been granted 93 US patents.
                      He is the associate editor of IEEE TNNLS, IEEE D&T, IEEE ESL, ACM JETC, and ACM TCPS, and served on the technical and organization committees of more than 40 international conferences.
                      He received 6 best paper awards and 12 best paper nominations from international conferences.
                      He is the recipient of NSF CAREER award and ACM SIGDA outstanding new faculty award.
                      He is the Fellow of IEEE.
                </p>
                <p>See <a href="https://rebootingcomputing.ieee.org/lpirc">LPIRC tournaments</a>.</p>
              </div>
              <div class="d-none d-lg-block col-lg-2 table-cell">
              </div>
          </div>

          <div class="row">
            <div class="d-none d-lg-block col-lg-2 table-cell">
                <h3>2:30pm—2:50pm</h3>
              </div>
              <div class="col-12 col-lg-8 table-cell">
                <h6 class="d-block d-lg-none">2:30pm—2:50pm</h6>
                <h3>"Real-Time Image Recognition Using Collaborative IoT Devices"</h3>
                <p><u>Ramyad Hadidi</u>, Jiashen Cao, Matthew Woodward, Michael S. Ryoo, Hyesoon Kim</p>
                <i>Georgia Institute of Technology, USA</i>
                <div class="d-block d-lg-none container container-wrapped-sm">
                    <div class="dvdt-card-links">
                      <a class="dvdt-a-github" href="https://github.com/ctuning/ck-request-asplos18-iot-farm">Validated</a>
                    </div>
                    <i>Nvidia Jetson TX2, Arm, Raspberry Pi, AlexNet, VGG16, TensorFlow, Keras, Avro</i>
                </div>
              </div>
              <div class="d-none d-lg-block col-lg-2 table-cell">
                <div class="dvdt-card-links">
                  <a class="dvdt-a-github" href="https://github.com/ctuning/ck-request-asplos18-iot-farm">Validated</a>
                </div>
                    <i>Nvidia Jetson TX2, Arm, Raspberry Pi, AlexNet, VGG16, TensorFlow, Keras, Avro</i>
              </div>
          </div>

          <div class="row">
              <div class="d-none d-lg-block col-lg-2 table-cell">
                <h3>2:50pm—3:10pm</h3>
              </div>
              <div class="col-12 col-lg-8 table-cell">
                <h6 class="d-block d-lg-none">2:50pm—3:10pm</h6>
                <h3>"Highly Efficient 8-bit Low Precision Inference of Convolutional Neural Networks with IntelCaffe"</h3>
                <p>Jiong Gong, <u>Haihao Shen</u>, Guoming Zhang, Xiaoli Liu, Shane Li, Ge Jin, Niharika Maheshwari</p>
                <i>Intel Corporation</i>
                <div class="d-block d-lg-none container container-wrapped-sm">
                    <div class="dvdt-card-links">
                      <a class="dvdt-a-github" href="https://github.com/ctuning/ck-request-asplos18-caffe-intel">Validated</a>
                    </div>
                      <i>Xeon Platinum 8124M, AWS, Intel C++ Compiler 17.0.5 20170817, ResNet-50, Inception-V3, SSD, 32-bit, 8-bit, Caffe</i>
                </div>
              </div>
              <div class="d-none d-lg-block col-lg-2 table-cell">
                <div class="dvdt-card-links">
                  <a class="dvdt-a-github" href="https://github.com/ctuning/ck-request-asplos18-caffe-intel">Validated</a>
                </div>
                    <i>Xeon Platinum 8124M, AWS, Intel C++ Compiler 17.0.5 20170817, ResNet-50, Inception-V3, SSD, 32-bit, 8-bit, Caffe</i>
              </div>
          </div>

          <div class="row">
              <div class="d-none d-lg-block col-lg-2 table-cell">
                <h3>3:10pm—3:30pm</h3>
              </div>
              <div class="col-12 col-lg-8 table-cell">
                <h6 class="d-block d-lg-none">3:10pm—3:30pm</h6>
                <h3>"VTA: Open Hardware/Software Stack for Vertical Deep Learning System Optimization"</h3>
                <p>Thierry Moreau</u>, Tianqi Chen, Luis Ceze</p>
                <i>University of Washington, USA</i>
                <div class="d-block d-lg-none container container-wrapped-sm">
                    <div class="dvdt-card-links">
                      <a class="dvdt-a-github" href="https://github.com/ctuning/ck-request-asplos18-mobilenets-tvm-arm">Validated</a>
                    </div>
                      <i>Xilinx FGPA (Pynq board), ResNet-*, MXNet, NNVM/TVM</i>
                </div>
              </div>
              <div class="d-none d-lg-block col-lg-2 table-cell">
                  <div class="dvdt-card-links">
                    <a class="dvdt-a-github" href="https://github.com/ctuning/ck-request-asplos18-mobilenets-tvm-arm">Validated</a>
                  </div>
                    <i>Xilinx FGPA (Pynq board), ResNet-*, MXNet, NNVM/TVM</i>
              </div>
          </div>

          <div class="row">
              <div class="d-none d-lg-block col-lg-2 table-title">
                <h3>3:30pm—4:00pm</h3>
              </div>
              <div class="col-12 col-lg-8 table-title">
                <h6 class="d-block d-lg-none">3:30pm—4:00pm</h6>
                <h3>Break</h3>
              </div>
              <div class="d-none d-lg-block col-lg-2 table-title">
              </div>
          </div>

          <div class="row">
              <div class="d-none d-lg-block col-lg-2 table-cell">
                <h3>4:00pm—4:20pm</h3>
              </div>
              <div class="col-12 col-lg-8 table-cell">
                <h6 class="d-block d-lg-none">4:00pm—4:20</h6>
                <h3>"Optimizing Deep Learning Workloads on Arm GPU with TVM"</h3>
                <p><u>Lianmin Zheng</u><sup>1</sup>, Tianqi Chen<sup>2</sup></p>
                <i><sup>1</sup> Shanghai Jiao Tong University, China<br>
                   <sup>2</sup> University of Washington, USA
                </i>
                <div class="d-block d-lg-none container container-wrapped-sm">
                    <div class="dvdt-card-links">
                      <a class="dvdt-a-github" href="https://github.com/ctuning/ck-request-asplos18-mobilenets-tvm-arm">Validated</a>
                    </div>
                      <i>Firefly-RK3399, GCC, LLVM, VGG16, MobileNet, ResNet-18, OpenBLAS vs ArmCL, MXNet, NNVM/TVM</i>
                </div>
              </div>
              <div class="d-none d-lg-block col-lg-2 table-cell">
                    <div class="dvdt-card-links">
                      <a class="dvdt-a-github" href="https://github.com/ctuning/ck-request-asplos18-mobilenets-tvm-arm">Validated</a>
                    </div>
                      <i>Firefly-RK3399, GCC, LLVM, VGG16, MobileNet, ResNet-18, OpenBLAS vs ArmCL, MXNet, NNVM/TVM</i>
              </div>
          </div>

          <div class="row">
              <div class="d-none d-lg-block col-lg-2 table-cell">
                <h3>4:20pm—4:50pm</h3>
              </div>
              <div class="col-12 col-lg-8 table-cell">
                <h6 class="d-block d-lg-none">4:20pm—4:50pm</h6>
                <h3>"Introducing open ReQuEST platform, scoreboard and long-term vision"</h3>
                <p><u>Grigori Fursin</u> and the ReQuEST organizers</p>
                <h3>"Exploring performance and accuracy of the MobileNets family using the Arm Compute Library"</h3>
                <p>Nikolay Chunosov, Flavio Vella, Anton Lokhmotov, <u>Grigori Fursin</u></p>
                <i>dividiti, UK<br>
                   cTuning foundation, France
                </i>
                <div class="d-block d-lg-none container container-wrapped-sm">
                    <div class="dvdt-card-links">
                      <a class="dvdt-a-github" href="https://github.com/dividiti/ck-request-asplos18-mobilenets-armcl-opencl">Validated</a>
                    </div>
                      <i>HiKey 960 (GPU), GCC, MobileNets exploration, ArmCL (18.01,18.02,dividiti optimizations), OpenCL</i>
                </div>
              </div>
              <div class="d-none d-lg-block col-lg-2 table-cell">
                  <div class="dvdt-card-links">
                    <a class="dvdt-a-github" href="https://github.com/dividiti/ck-request-asplos18-mobilenets-armcl-opencl">Validated</a>
                  </div>
                    <i>HiKey 960 (GPU), GCC, MobileNets exploration, ArmCL (18.01,18.02,dividiti optimizations), OpenCL</i>
              </div>
          </div>

          <div class="row">
              <div class="d-none d-lg-block col-lg-2 table-cell">
                <h3>5:00pm</h3>
              </div>
              <div class="col-12 col-lg-8 table-cell">
                <h6 class="d-block d-lg-none">5:00pm</h6>
                <h3>"Tackling complexity, reproducibility and tech transfer challenges in a rapidly evolving AI/ML/systems research"</h3>
                <p>Moderators: <a href="http://fursin.net/research.html">Grigori Fursin</a> and <a href="https://homes.cs.washington.edu/~moreau">Thierry Moreau</a>.</p>
                <h3>"Exploring performance and accuracy of the MobileNets family using the Arm Compute Library"</h3>
                <p>
                  We plan to center discussion around the following questions:
                  <ul>
                   <li>
                    How do we facilitate tech transfer between academia and industry in a quickly evolving research landscape?
                   <li>
                    How do we incentivize companies and academic researchers to release more artifacts and open source projects as portable, customizable and reusable components which can be collaboratively optimized by the community across diverse models, data sets and platforms from the cloud to edge?
                   <li>
                    How do we ensure reproducible evaluation and fair comparison of diverse AI/ML frameworks, libraries, techniques and tools?
                   <li>
                    What other workloads (AI, ML, quantum) and exciting research challenges should ReQuEST attempt to solve in its future iterations with the help of the multi-disciplinary community: reducing training time and costs, comparing specialized hardware (TPU/FPGA/DSP), distributing learning across edge devices, ...
                  </ul>
                </p>
                <h3 class="mt-5">Participants:</h3>
                <p><b>Hillery Hunter, IBM</b></p>
                <img class="table-img" src="_resources/photo-hilary-hunter.jpg" style="float:right;padding:0 0 8px 24px;">
                <p><a href="https://researcher.watson.ibm.com/researcher/view.php?person=us-hhunter">Hillery Hunter</a>
                    is an IBM Fellow and Director of the Accelerated Cognitive Infrastructure group at IBM's T.J. Watson Research Center in Yorktown Heights, NY.
                    She is interested in cross-disciplinary technology topics, spanning silicon to system architecture to achieve new solutions to traditional problems.
                    Her team pursues hardware-software co-optimization to take the wait time out of machine and deep learning problems.
                    Her prior work was in the areas of DRAM main memory systems and embedded DRAM, and she gained development experience serving as IBM's server and mainframe DDR3-generation end-to-end memory power lead.
                    In 2010, she was selected by the National Academy of Engineering for its Frontiers in Engineering Symposium, a recognition as one of the top young engineers in America.
                    Dr. Hunter received the Ph.D. degree in Electrical Engineering from the University of Illinois, Urbana-Champaign and is a member of the IBM Academy of Technology.
                    Hillery was appointed as an IBM Fellow in 2017.
                </p>
                <br>
                <p><b>Yiran Chen, Duke University</b></p>
                <img class="table-img" src="_resources/photo-yiran-chen-reduced-min.jpg" style="float:right;padding:0 0 8px 24px;">
                <p><a href="http://ei-lab.org/people/faculty/yiran-chen-3/">Yiran Chen</a> received B.S and M.S. from Tsinghua University and Ph.D. from Purdue University in 2005.
                    After five years in industry, he joined University of Pittsburgh in 2010 as Assistant Professor and then promoted to Associate Professor with tenure in 2014, held Bicentennial Alumni Faculty Fellow.
                    He now is a tenured Associate Professor of the Department of Electrical and Computer Engineering at Duke University and serving as the co-director of Duke Center for Evolutionary Intelligence (CEI), focusing on the research of new memory and storage systems, machine learning and neuromorphic computing, and mobile computing systems.
                    Dr. Chen has published one book and more than 300 technical publications and has been granted 93 US patents.
                    He is the associate editor of IEEE TNNLS, IEEE D&T, IEEE ESL, ACM JETC, and ACM TCPS, and served on the technical and organization committees of more than 40 international conferences.
                    He received 6 best paper awards and 12 best paper nominations from international conferences.
                    He is the recipient of NSF CAREER award and ACM SIGDA outstanding new faculty award. He is the Fellow of IEEE.
                </p>
                <br>
                <p><b>Charles Qi, Cadence</b></p>
                <img class="table-img" src="_resources/photo-charles-qi-reduced-min.jpg" style="float:right;padding:0 0 8px 24px;">
                <p><a href="https://www.linkedin.com/in/charlesqi">Charles Qi</a> is a system solutions architect in Cadence's IPG System and Software team, responsible for providing vision system solutions based on the Cadence(R) Tensilica Vision DSP technology and a broad range of interface IP portfolio.
                    At system level, his primary focus is image sensing, computer vision and deep learning hardware and software for high-performance automotive vision ADAS SoC.
                    Currently he is also an active internal architecture team member for high performance neural network acceleration hardware IPs.
                    Prior to joining Cadence, Charles held various technical positions in Intel, Broadcom and several high-tech startups.
                </p>
              </div>
              <div class="d-none d-lg-block col-lg-2 table-cell">
              </div>
          </div>

      </div>

        <div class="container">
            <h2 class="pt-5">Important dates</h2>
            <ul>
            <!-- <li>Informal intent to submit: <b>February 7, 2018 AoE</b><br>
                 &nbsp;&nbsp;<i>Please send us an <a href="mailto:moreau@cs.washington.edu;Grigori.Fursin@cTuning.org">email</a>
                 briefly describing your submission including hardware/software dependencies
                 and optimization metrics. We will start helping you convert your artifacts
                 to the <a href="http://cKnowledge.org">Collective Knowledge</a> format.</i> -->
             <li>Artifact submissions due: <b><strike>February 12, 2018 AoE</strike></b><br>
             <li>Artifact evaluation: <b><strike>February 13-February 21, 2018</strike></b>
             <li>Author notification: <b><strike>February  22, 2018</strike></b>
             <li>ASPLOS early registration deadline: <b><strike>February 23, 2018</strike></b>
                 (See <a href="https://www.asplos2018.org/registration/#reg">ASPLOS registration</a>
                 and <a href="https://www.asplos2018.org/registration/#visa">visa support</a>)
             <li>Workshop with presentations and discussions of winning workflows: <b><strike>March 24, 2018</strike></b>
             <li>Final papers and artifacts for the Digital Library: <b><strike>mid April, 2018</strike></b>
             <li>Report to our advisory board: <b><strike>end of April, 2018</strike></b>
            </ul>
        </div>

        <div class="container">
            <h2 class="pt-5">Call for submissions</h2>
            <p>
                The 1st ReQuEST tournament is co-located with <a href="https://www.asplos2018.org">ACM ASPLOS'18</a> and will focus on optimizing the whole model/software/hardware stack for image classification based on the <a href="http://www.image-net.org/challenges/LSVRC"><b>ImageNet Large Scale Visual Recognition Challenge (ILSVRC)</b></a>.
                Unlike the classical ILSVRC where submissions are ranked according to their classification accuracy, however, ReQuEST submissions will be evaluated according to multiple metrics and trade-offs selected by the authors (e.g. accuracy, speed, throughput, energy consumption, hardware cost, usage cost, etc.) in a unified, reproducible and objective way using the <a href="https://github.com/ctuning/ck">Collective Knowledge</a> framework (CK).
                Restricting the competition to a single application domain will allow us to test our open-source ReQuEST tournament infrastructure, validate it across multiple platforms and environments, and prepare a dedicated live scoreboard with results similar to this <a href="https://cknow.io/c/result/pareto-efficient-ai-co-design-tournament-request-acm-asplos-2018">open SOTA scoreboard</a>.
            </p>

            <p>
                We encourage participants to target accessible, off-the-shelf hardware to allow our evaluation committee to conveniently reproduce their results. Example systems include:
            </p>
            <ul>
               <li>Server-class: AWS/Azure cloud instance, any x86-based desktop system.</li>
               <li>Mobile-class: Any Arm-based (e.g. NVIDIA Jetson TX2, Raspberry Pi 3, Xilinx PYNQ board), or Intel-Atom-based SoC development board, Android-based smartphone or tablets.</li>
               <li>IoT-class: Low-power Arm micro-controllers (e.g. Freescale FRDM KL03 Development Board).</li>
            </ul>
            <p>
                If a submission relies on an exotic hardware platform, the participants can either provide restricted access to their evaluation platform to the artifact evaluation committee, or notify the organizers in advance (please try to give us at least 3 weeks notice) about their choice so that a similar platform can be acquired in time (assuming the cost is not prohibitive).
            </p>
            <p>Example optimizations include:</p>
            <ul>
               <li>Design space exploration of model topologies, operators, activation functions, configurations.</li>
               <li>Hyper-parameter search and meta-learning techniques that help optimize accuracy and inference time.</li>
               <li>Comparison of different deep learning systems (for example TensorFlow vs. Caffe2 vs. CNTK vs. MXNet).</li>
               <li>Model optimizations that trade accuracy for speed or efficiency (e.g. reduced precision and model compression).</li>
               <li>Operator-level quantization, binarization or ternarization techniques to improve overall inference time (e.g. binary networks, XNOR nets).</li>
               <li>Library optimizations targeting deep learning operators on mobile systems (e.g. depthwise convolution).</li>
               <li>FPGA acceleration that takes advantage of narrow integer bitwidth.</li>
               <li>Software optimizations targeting GPU-less mobile/IoT systems.</li>
            </ul>
            <p>
                <i class="alert-danger">We strongly encourage artifact submissions for already published optimization techniques since one of the ReQuEST goals is to prepare a reference (baseline) set of implementations of various algorithms shared as portable, customizable and reusable CK components with a common API.
                In fact, the ReQuEST submissions will be directly fed into <a href="https://www.acm.org/publications/dl-pilot-integrations">pilot CK integrations with the ACM Digital Library</a>.</i>
            </p>
        </div>

        <div class="container">
            <h2 class="pt-5">Submission</h2>
            <p>
                We follow <a href="https://ctuning.org/ae/submission.html">standard procedures</a> for submitting and evaluating experimental workflows, as established at leading systems conferences including CGO, PPoPP, PACT and SuperComputing ("artifact evaluation"):
            </p>
            <ul>
                <li>
                  <strong>Step 1: Share your experimental artifacts and workflows</strong>
                  <p>
                     You should make all artifacts and experimental workflows publicly available via GitHub, GitLab, Bitbucket or similar, or pack them in a zip/tar archive or Docker/VM image.
                     You should also provide instructions and scripts to build and run your workflows on a target platform, measure the characteristics and compare the results against a reference implementation.
                     <br>
                     If you are already familiar with the open-source <a href="http://github.com/ctuning/ck">Collective Knowledge</a> framework (CK), you are encouraged to convert your experimental workflows to to <a href="https://github.com/ctuning/ck/wiki/Portable-workflows">portable CK workflows</a>.
                     Such workflows can automatically set up the environment, detect required software dependencies, install missing packages and run experiments, thus automating artifact evaluation.
                     (See some examples <a href="https://cknow.io/demo">here</a>.)
                     <br>
                     If you are not familiar with CK, worry not! We will gladly help you
                     convert your submission to CK during the <a href="#evaluation">evalution
                     stage</a>.
                  </p>
                </li>
               <li>
                 <strong>Step 2: Submit an extended abstract with Artifact Appendix</strong>
                 <p>
                     You should prepare an extended abstract (max 4 pages) using this <a href="https://cKnowledge.org/request/request-template.tex">ReQuEST LaTex template</a> 
                     in the <a href="http://www.sigplan.org/sites/default/files/sigplanconf.cls">SIGPLAN conference style</a>.
                     <br>
                     Include your name, affiliation, and a brief description of your work (which can be novel or already presented elsewhere).
                     Please also fill in the Artifact Appendix in the above template, including how to obtain your artifacts and workflows. Provide a detailed specification of your experimental workflow, a list of optimization metrics (speed, accuracy, energy, costs, etc.) and the expected results (which the reviewers will need to independently validate).
                     <br>
                     Please submit your extended abstract as a PDF via the <a href="https://asplos18request.hotcrp.com">ReQuEST HotCRP website</a>.
                     Please contact the <a href="mailto:moreau@cs.washington.edu;Grigori.Fursin@cTuning.org">organizers</a> if your encounter any problems.
                 </p>
               </li>
           </ul>
        </div>

        <div class="container">
           <h2 class="pt-5">Evaluation</h2>
           <p>
               ReQuEST is backed by the
               <a href="https://www.acm.org/publications/task-force-on-data-software-and-reproducibility">ACM Task Force on Data, Software, and Reproducibility in Publication</a>
               and uses the standard
               <a href="https://cTuning.org/ae/reviewing.html">artifact evaluation methodology</a>.
               Artifact evaluation is single blind (see <a href="https://cTuning.org/ae">PPoPP, CGO, PACT, RTSS and SuperComputing</a>).
               Reviews will be performed by the <a href="#organizers">organizers</a> and volunteers ("reviewers"), and can be made public upon the authors' request (see
               <a href="http://adapt-workshop.org/submission2016.html">ADAPT</a>).
               Quality and efficiency metrics will be collected for each submission, and displayed on a live ReQuEST scoreboard similar
               to this <a href="http://cKnowledge.org/repo">open CK repository</a>.
           </p>
           <ul>
                <li>
                  <strong>Step 1: Collaborate on converting your workflows to CK</strong>
                  <p>
                     If your submission is not in the <a href="https://github.com/ctuning/ck">CK format</a>, we will help you to add a portable CK workflow for your algorithm while reusing available CK packages and modules shared by the community (see <a href="https://github.com/ctuning/ck/wiki">CK Getting Started Guides</a>, <a href="https://github.com/dividiti/ck-request-asplos18-mobilenets-armcl-opencl">CK ReQuEST workflow example to explore MobileNets on Arm GPUs</a>, <a href="https://github.com/ctuning/ck/wiki/Shared-packages">shared CK packages</a> <a href="https://github.com/ctuning/ck/wiki/Shared-soft-descriptions">CK software detection plugins</a>, <a href="https://github.com/ctuning/ck/wiki/Shared-modules">reusable CK modules (unified scripts and tool wrappers)</a> and <a href="https://github.com/ctuning/ck/wiki/Shared-repos#user-content-shared-workflows">CK repositories with AI/ML workflows</a>).
                     You may choose how to communicate with us during this step: either privately via HotCRP, semi-privately via a <a href="https://request-workshop.slack.com">dedicated Slack channel with all authors and reviewers</a>, or, <a href="http://adapt-workshop.org/motivation2016.html">preferably</a>, publicly via <a href="https://collective-knowledge.slack.com">CK slack channel</a> or the <a href="https://groups.google.com/forum/#!forum/collective-knowledge">CK mailing list</a> (thus making the community immediately aware of your artifact).
                  </p>
                </li>
                <li>
                  <strong>Step 2: Collaborate on validating your results</strong>
                  <p>
                     We will form a ReQuEST artifact evaluation committee (AEC) from the organizers and volunteers (&quot;reviewers&quot;).
                     The AEC task is to objectively evaluate submissions on appropriate hardware platforms, reproduce results and aggregate them on a multi-objective public scoreboard.
                     AE will be a friendly and interactive process between the authors and the reviewers, with the goal of making the artifacts as useful as possible for the community.
                     For example, the reviewers may encounter some unexpected problems, and ask the authors for help to fix them.
                     <br>
                     Again, the authors can communicate with the reviewers privately via HotCRP, semi-privately via <a href="https://request-workshop.slack.com">Slack</a>, or publicly by opening tickets in shared repositories (see examples <a href="https://github.com/thu-pacman/self-checkpoint/issues/1">1</a> and <a href="https://gitlab.com/michel-steuwer/cgo_2017_artifact/issues/1">2</a>) and/or via the <a href="https://groups.google.com/forum/#!forum/collective-knowledge">CK mailing list</a>.
                     If any of the organizers submit their workflows (mainly to provide reference implementations), their submissions will go through public evaluation.
                  </p>
                </li>
                <li>
                  <strong>Step 3: Collaborate on visualizing your results on a public scoreboard</strong>
                  <p>
                     Due to the multi-faceted nature of the competition, submissions will not be ranked according to a single metric (as this often results in over-engineered solutions), but instead the AEC will assess their Pareto optimality on two or more metrics exposed by the authors.
                     As such, there will not be a single winner, but rather better and worse designs based on their relative Pareto optimality (up to 3 design points allowed per each submission).
                     We will collaborate with the authors to correctly visualize the results and SW/HW/model configurations on a  <a href="http://cKnowledge.org/repo">public scoreboard</a> while grouping them according to certain categories of their choice (e.g. embedded vs. server).
                     A unique submission may define a category in its own right.
                     To win, the results of an entry will normally lie close to the Pareto-optimal frontier in its category.
                     However, a winning entry can be also praised for its originality, reproducibility, adaptability, scalability, portability, ease of use, etc.
                  </p>
                </li>
            </ul>
        </div>

        <div class="container">
            <h2 class="pt-5">Presentation</h2>
            <ul>
                 <li>
                   <strong>Step 1: Present at the ReQuEST workshop at ASPLOS'18</strong>
                   <p>
                      We will announce accepted SW/HW/model configurations at the end of February, and invite the authors to present their work at the 1st ReQuEST workshop co-located with <a href="https://www.asplos2018.org">ASPLOS 2018</a> (ACM conference on Architectural Support for Programming Languages and Operating Systems, which is the premier forum for multidisciplinary systems research spanning computer architecture and hardware, programming languages and compilers, operating systems and networking).
                      This will give the authors an opportunity to share their research and implementation insights with the research community as well as discuss future R&amp;D directions.
                      <br>
                      A common academic and industrial panel will be held at the end of the workshop to discuss how to improve the common SW/HW co-design methodology and infrastructure for deep learning and other real-world workloads.
                   </p>
                 </li>
                 <li>
                   <strong>Step 2: Publish in the ACM Digital Library</strong>
                   <p>
                     The authors of the winning submissions will publish their extended abstracts with an Artifact Appendix and related artifacts in the <a href="http://dl.acm.org">ACM Digital Library</a> (even if their techniques have already been published, since the workshop focuses on validated and reusable artifacts!)
                     <i>Furthermore, we have partnered with <a href="http://acm.org">ACM</a> to award <a href="https://www.acm.org/publications/policies/artifact-review-badging">&quot;available / reusable / replicated&quot; badges</a> to all the winning artifacts.
                     This will make them discoverable via the ACM Digital Library (check this out by selecting &quot;Artifact Badge&quot; for a field and then select any badge you wish in the <a href="https://dl.acm.org/advsearch.cfm?coll=DL&dl=ACM">ACM DL advanced search</a>)!</i>
                   </p>
                 </li>
             </ul>

            <center>
                 <img src="https://www.acm.org/binaries/content/gallery/acm/publications/replication-badges/artifacts_available_dl.jpg" width="50">
                 <img src="https://www.acm.org/binaries/content/gallery/acm/publications/replication-badges/artifacts_evaluated_reusable_dl.jpg" width="50">
                 <img src="https://www.acm.org/binaries/content/gallery/acm/publications/replication-badges/results_replicated_dl.jpg" width="50">
            </center>

        </div>
              <h3 class="pt-3 pt-lg-5">Advisory/industrial board goal</h3>
              <p>
                  Members of the REQUEST advisory/industrial board will look over and comment on the results of our tournaments and workshops, collaborate on a common methodology for reproducible evaluation and optimization, suggest realistic workloads for future tournaments, arrange access to rare hardware to Artifact Evaluation Committee, and provide prizes for the most efficient solutions.
              </p>


              <h3 class="pt-3 pt-lg-5">Open research goal</h3>
              <p>
                  REQUEST attempts to put systems researchers, application engineers and end-users on the same ground by providing a common and portable evaluation framework while sharing all artifacts and optimization results in an open and reproducible way.
                  We expect that our open repository with customizable, reusable and optimized artifacts will be useful for
              </p>
              <ul>
                  <li>
                    scientists to accelerate their research by picking up the most efficient, resource-aware and input-adaptable solutions for their algorithms;
                  </li>
                  <li>
                    SW/HW researchers to reuse, improve and build upon each others' work (main pillars of open science) thus accelerating machine learning and systems research;
                  </li>
                  <li>
                    system designers and integrators to accelerate development of the next generation of efficient hardware and software for emerging workloads such as deep learning using publicly validated optimization results.
                  </li>
              </ul>
              <p>
                  Feel free to <a href="mailto:moreau@cs.washington.edu;Grigori.Fursin@cTuning.org">contact us</a> if you have questions or suggestions!
              </p>
          </div>

