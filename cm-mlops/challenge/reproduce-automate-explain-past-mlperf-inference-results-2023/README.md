### Challenge

MLPerf submitters usually report the best results but rarely explain how they were achieved.

We invite researchers, engineers and students to reproduce the top past MLPerf inference results,
analyze and explain performance or power efficiency improvements in comparison with unoptimized
reference implementations and add CM automation interface. 

A detailed experience report must be provided - if accepted, you will be able to present your findings 
at the upcoming HiPEAC'23 workshop and other events that we are organizing with ACM, IEEE and HiPEAC.

Join our public [Discord server](https://discord.gg/JjWNWXKxwT) and/or
our [weekly conf-calls](https://docs.google.com/document/d/1zMNK1m_LhWm6jimZK6YE05hu4VH9usdbKJ3nBy-ZPAw/edit)
to discuss this challenge with the organizers.

Read [this documentation](https://github.com/mlcommons/ck/blob/master/docs/mlperf/inference/README.md) 
to run reference implementations of MLPerf inference benchmarks 
using the CM automation language and use them as a base for your developments.

Check [this ACM REP'23 keynote](https://doi.org/10.5281/zenodo.8105339) to learn more about our open-source project and long-term vision.

### Prizes

* *All contributors will be able to present their findings at the HiPEAC workshop on reproducibility and participate in writing a common white paper about running and comparing MLPerf inference benchmarks.*
* *All contributors will receive 2 point*
* *All contributors will receive an official MLCommons Collective Knowledge contributor award (see [this example](https://ctuning.org/awards/ck-award-202307-zhu.pdf)).*
* *The first 3 submitters will receive a prize of 150$.*


#### Organizers

* [HiPEAC](https://hipeac.net)
* [ACM REP'23](https://acm-rep.github.io/2023/)
* [cTuning foundation](https://cTuning.org)
* [cKnowledge Ltd](https://cKnowledge.org)
