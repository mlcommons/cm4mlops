### Challenge

MLPerf submitters usually report the best performance/power numbers but rarely explain how they were achieved.

We invite researchers, engineers and students to reproduce any top past MLPerf inference results,
analyze and explain how performance or power efficiency is achieved in comparison with unoptimized
reference implementations, and add CM automation interface to make it easier to reproduce and replicate
them by the community. 

A detailed experience report must be provided - if accepted, you will be able to present your findings 
at the upcoming HiPEAC'23 workshop and our upcoming white paper with MLCommons.

Join our public [Discord server](https://discord.gg/JjWNWXKxwT) to discuss this challenge with the organizers.

Read [this documentation](https://github.com/mlcommons/ck/blob/master/docs/mlperf/inference/README.md) 
to run reference implementations of MLPerf inference benchmarks 
using the CM automation language and use them as a base for your developments.

Check [this ACM REP'23 keynote](https://doi.org/10.5281/zenodo.8105339) to learn more about our open-source project and long-term vision.

### Prizes

* *All contributors will be able to present their findings at the HiPEAC workshop on reproducibility and participate in writing a common white paper about running and comparing MLPerf inference benchmarks.*
* *All contributors will receive 2 points*
* *All contributors will receive an official MLCommons Collective Knowledge contributor award (see [this example](https://ctuning.org/awards/ck-award-202307-zhu.pdf)).*


#### Organizers

* [cTuning foundation](https://cTuning.org)
* [cKnowledge Ltd](https://cKnowledge.org)
* [HiPEAC](https://hipeac.net)
* [ACM REP](https://acm-rep.github.io)
* [MLCommons](https://mlcommons.org)
