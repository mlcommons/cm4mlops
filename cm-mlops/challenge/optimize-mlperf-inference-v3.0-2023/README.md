### Challenge

Prepare, optimize and reproduce MLPerf inference v3.0 benchmarks across diverse implementations, software and hardware
using the [MLCommons CK framework](https://github.com/mlcommons/ck):

* [GUI to run MLPerf inference benchmarks](https://cknowledge.org/mlperf-inference-gui)
* [GUI to prepare MLPerf inference submissions](https://cknowledge.org/mlperf-inference-submission-gui)

Join this public [Discord server](https://discord.gg/JjWNWXKxwT) to discuss with the community and organizers
how to use and enhance CK to benchmark and optimize ML Systems.

### Organizers

* [MLCommons taskforce on automation and reproducibility](https://cKnowledge.org/mlcommons-taskforce)
* [cTuning foundation](https://cTuning.org)
* [cKnowledge](https://cKnowledge.org)

### Status

This challenge has been successfully completed.

### Results

All results are available in this [GitHub repo](https://github.com/ctuning/cm_inference_results)
and can be visualized and compared using the [MLCommons Collective Knowledge Playground](https://access.cknowledge.org/playground/?action=experiments&tags=mlperf-inference,v3.0).
